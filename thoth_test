#!/usr/bin/env -S uv run --script
# /// script
# requires-python = ">=3.11"
# dependencies = ["rich>=13.7", "click>=8.0"]
# ///
"""
Thoth Test Suite - Comprehensive regression tests for Thoth CLI

This test suite validates all aspects of Thoth's functionality through
black-box testing using subprocess. Each test validates:
- Exit codes
- Standard output patterns
- Standard error patterns  
- File creation and content
"""

import subprocess
import json
import os
import sys
import tempfile
import time
import signal
from dataclasses import dataclass, field
from pathlib import Path
from typing import List, Optional, Dict, Any, Tuple, Union
import re
import shutil
from datetime import datetime
from uuid import uuid4

from rich.console import Console
from rich.table import Table
from rich.progress import Progress, SpinnerColumn, TextColumn, BarColumn
import click

# Initialize console for output
console = Console()

# Test configuration
THOTH_EXECUTABLE = "./thoth"
TEST_TIMEOUT = 30
CLEANUP_ENABLED = True
VERBOSE_OUTPUT = False  # Global verbose flag

# ============================================================================
# Data Models
# ============================================================================

@dataclass
class TestResult:
    """Result of a single test execution"""
    test_id: str
    passed: bool
    duration: float
    stdout: str
    stderr: str
    exit_code: int
    error_message: Optional[str] = None
    created_files: List[Path] = field(default_factory=list)
    is_api_key_failure: bool = False

@dataclass
class TestCase:
    """Definition of a test case"""
    test_id: str
    description: str
    command: List[str]
    env: Dict[str, str] = field(default_factory=dict)
    expected_exit_code: int = 0
    expected_stdout_patterns: List[str] = field(default_factory=list)
    expected_stderr_patterns: List[str] = field(default_factory=list)
    not_expected_stdout_patterns: List[str] = field(default_factory=list)
    not_expected_stderr_patterns: List[str] = field(default_factory=list)
    expected_files: List[str] = field(default_factory=list)
    timeout: int = TEST_TIMEOUT
    cleanup_files: List[str] = field(default_factory=list)
    skip: bool = False
    skip_reason: str = ""
    provider: Optional[str] = None  # Which provider this test requires
    api_key_method: str = "env"  # "env", "cli", or "config"
    signal_after: Optional[float] = None  # Send signal after N seconds

# ============================================================================
# Utility Functions
# ============================================================================

def generate_mock_api_key() -> str:
    """Generate a random dummy API key for mock provider testing"""
    return f"mock-key-{uuid4().hex[:16]}"

def get_test_api_keys() -> Dict[str, str]:
    """Get API keys for testing (real or generated)"""
    return {
        "mock": generate_mock_api_key(),
        "openai": os.getenv("OPENAI_API_KEY", ""),
        "perplexity": os.getenv("PERPLEXITY_API_KEY", "")
    }

def get_available_providers(test_api_keys: Dict[str, str]) -> List[str]:
    """Determine which providers are available for testing"""
    providers = ["mock"]  # Mock is always available
    
    if test_api_keys.get("openai"):
        providers.append("openai")
    
    if test_api_keys.get("perplexity"):
        providers.append("perplexity")
    
    return providers

def run_command(command: List[str], env: Optional[Dict[str, str]] = None, 
                timeout: int = TEST_TIMEOUT, cwd: Optional[str] = None,
                signal_after: Optional[float] = None) -> Tuple[int, str, str]:
    """
    Execute a command and return exit code, stdout, stderr
    """
    # Merge environment variables
    cmd_env = os.environ.copy()
    if env:
        cmd_env.update(env)
    
    # Set terminal columns to ensure tables render properly
    cmd_env["COLUMNS"] = "200"
    
    try:
        # Use Popen for more control over output capture
        process = subprocess.Popen(
            command,
            stdout=subprocess.PIPE,
            stderr=subprocess.PIPE,
            text=True,
            env=cmd_env,
            cwd=cwd,
            bufsize=-1  # Use system default buffering
        )
        
        try:
            if signal_after:
                # Wait for specified time then send signal
                import threading
                def send_signal():
                    time.sleep(signal_after)
                    if process.poll() is None:  # Process still running
                        process.send_signal(signal.SIGINT)
                
                signal_thread = threading.Thread(target=send_signal)
                signal_thread.start()
            
            stdout, stderr = process.communicate(timeout=timeout)
            return process.returncode, stdout, stderr
        except subprocess.TimeoutExpired:
            process.kill()
            stdout, stderr = process.communicate()
            return -1, stdout, f"Command timed out after {timeout} seconds\n{stderr}"
            
    except Exception as e:
        return -1, "", f"Command failed: {str(e)}"

def cleanup_test_files(patterns: List[str]):
    """Remove test artifacts matching patterns"""
    if not CLEANUP_ENABLED:
        return
    
    for pattern in patterns:
        for file_path in Path(".").glob(pattern):
            try:
                if file_path.is_file():
                    file_path.unlink()
                elif file_path.is_dir():
                    shutil.rmtree(file_path)
            except Exception:
                pass

def validate_file_creation(expected_files: List[str]) -> Tuple[bool, List[Path], str]:
    """Check if expected files were created"""
    created_files = []
    missing_files = []
    
    for file_pattern in expected_files:
        matching_files = list(Path(".").glob(file_pattern))
        if matching_files:
            created_files.extend(matching_files)
        else:
            missing_files.append(file_pattern)
    
    if missing_files:
        return False, created_files, f"Missing expected files: {', '.join(missing_files)}"
    
    return True, created_files, ""

def validate_output(output: str, expected_patterns: List[str], 
                   not_expected_patterns: List[str]) -> Tuple[bool, str]:
    """Validate output against expected and not-expected patterns"""
    # Check expected patterns
    for pattern in expected_patterns:
        if not re.search(pattern, output, re.MULTILINE | re.DOTALL):
            return False, f"Pattern not found: {pattern}"
    
    # Check not-expected patterns
    for pattern in not_expected_patterns:
        if re.search(pattern, output, re.MULTILINE | re.DOTALL):
            return False, f"Unexpected pattern found: {pattern}"
    
    return True, ""

def is_api_key_error(stdout: str, stderr: str) -> bool:
    """Check if the error is due to missing API key"""
    error_patterns = [
        r"API key not found",
        r"api key not found",
        r"Set.*API_KEY",
        r"OPENAI_API_KEY",
        r"PERPLEXITY_API_KEY"
    ]
    combined_output = stdout + stderr
    return any(re.search(pattern, combined_output, re.IGNORECASE) for pattern in error_patterns)

def create_test_config(config_dir: Path, content: str):
    """Create a test configuration file"""
    config_dir.mkdir(parents=True, exist_ok=True)
    config_file = config_dir / "config.toml"
    config_file.write_text(content)
    return config_file

def find_latest_output_file(pattern: str = "*_*_*_*_*.md") -> Optional[Path]:
    """Find the most recently created output file matching pattern"""
    files = sorted(Path(".").glob(pattern), key=lambda p: p.stat().st_mtime, reverse=True)
    return files[0] if files else None

def validate_test_environment(test_api_keys: Dict[str, str], requested_providers: Optional[List[str]] = None) -> None:
    """Validate test environment and show configuration"""
    console.print("\n[bold]Test Environment Configuration:[/bold]")
    
    # Always show mock status (it's always available)
    console.print(f"Mock API Key: {test_api_keys['mock'][:8]}... (auto-generated)")
    
    # If specific providers requested, validate only those
    if requested_providers:
        missing_keys = []
        for provider in requested_providers:
            if provider == "mock":
                continue  # Mock is always available
            elif provider == "openai":
                if test_api_keys['openai']:
                    console.print(f"OpenAI API Key: {test_api_keys['openai'][:8]}... ✓")
                else:
                    console.print("[red]OpenAI API Key: Not set (required for requested tests)[/red]")
                    missing_keys.append("OPENAI_API_KEY")
            elif provider == "perplexity":
                if test_api_keys['perplexity']:
                    console.print(f"Perplexity API Key: {test_api_keys['perplexity'][:8]}... ✓")
                else:
                    console.print("[red]Perplexity API Key: Not set (required for requested tests)[/red]")
                    missing_keys.append("PERPLEXITY_API_KEY")
        
        if missing_keys:
            console.print(f"\n[red]Error: Missing required API keys: {', '.join(missing_keys)}[/red]")
            console.print("Please set the environment variables or remove these providers from your selection.")
            sys.exit(1)
    else:
        # Default behavior: show status of all providers
        if test_api_keys['openai']:
            console.print(f"OpenAI API Key: {test_api_keys['openai'][:8]}... ✓")
        else:
            console.print("[yellow]OpenAI API Key: Not set (OpenAI tests will be skipped)[/yellow]")
        
        if test_api_keys['perplexity']:
            console.print(f"Perplexity API Key: {test_api_keys['perplexity'][:8]}... ✓")
        else:
            console.print("[yellow]Perplexity API Key: Not set (Perplexity tests will be skipped)[/yellow]")
    
    console.print("")  # Blank line

# ============================================================================
# Test Runner
# ============================================================================

class TestRunner:
    """Executes test cases and validates results"""
    
    def __init__(self, verbose: bool = False, save_output: bool = False, requested_providers: Optional[List[str]] = None):
        self.results: List[TestResult] = []
        self.start_time = time.time()
        self.test_api_keys = get_test_api_keys()
        self.available_providers = get_available_providers(self.test_api_keys)
        self.requested_providers = requested_providers
        self.skipped_by_provider = {"openai": 0, "perplexity": 0}
        self.verbose = verbose
        self.save_output = save_output
        if save_output:
            self.output_dir = Path("test_outputs") / datetime.now().strftime("%Y%m%d_%H%M%S")
            self.output_dir.mkdir(parents=True, exist_ok=True)
    
    def should_skip_test(self, test_case: TestCase) -> Tuple[bool, str]:
        """Check if test should be skipped based on provider availability and selection"""
        if not test_case.provider:
            return False, ""  # No specific provider required
        
        # If specific providers were requested, skip tests for other providers
        if self.requested_providers and test_case.provider not in self.requested_providers:
            return True, f"Provider {test_case.provider} not requested"
        
        if test_case.provider == "mock":
            return False, ""  # Mock tests always run (API key auto-generated)
        
        # Check if API key is available for the provider
        if test_case.provider not in self.available_providers:
            self.skipped_by_provider[test_case.provider] = self.skipped_by_provider.get(test_case.provider, 0) + 1
            return True, f"{test_case.provider.upper()}_API_KEY not set"
        
        return False, ""
    
    def run_test(self, test_case: TestCase) -> TestResult:
        """Execute a single test case"""
        # Check for manual skip
        if test_case.skip:
            return TestResult(
                test_id=test_case.test_id,
                passed=True,
                duration=0,
                stdout="",
                stderr=f"Skipped: {test_case.skip_reason}",
                exit_code=0
            )
        
        # Check for provider-based skip
        should_skip, skip_reason = self.should_skip_test(test_case)
        if should_skip:
            return TestResult(
                test_id=test_case.test_id,
                passed=True,
                duration=0,
                stdout="",
                stderr=f"Skipped: {skip_reason}",
                exit_code=0
            )
        
        start_time = time.time()
        
        # Clean up any existing test files
        cleanup_test_files(test_case.cleanup_files)
        
        # Set up environment with appropriate API key
        test_env = test_case.env.copy()
        test_command = test_case.command.copy()
        
        if test_case.provider:
            api_key = self.test_api_keys.get(test_case.provider, "")
            
            if test_case.api_key_method == "env":
                # Inject the appropriate API key in environment
                if test_case.provider == "mock":
                    # Don't override if already set in test_env
                    if "MOCK_API_KEY" not in test_env:
                        test_env["MOCK_API_KEY"] = api_key
                elif test_case.provider == "openai":
                    if "OPENAI_API_KEY" not in test_env:
                        test_env["OPENAI_API_KEY"] = api_key
                elif test_case.provider == "perplexity":
                    if "PERPLEXITY_API_KEY" not in test_env:
                        test_env["PERPLEXITY_API_KEY"] = api_key
                    
            elif test_case.api_key_method == "cli":
                # Add API key as command-line argument
                if "--api-key" not in test_command:
                    test_command.extend(["--api-key", api_key])
                else:
                    # Replace existing dummy key with actual key
                    idx = test_command.index("--api-key")
                    if idx + 1 < len(test_command):
                        test_command[idx + 1] = api_key
                        
            elif test_case.api_key_method == "config":
                # Create a temporary config file with API key
                config_content = f"""version = "1.0"

[general]
default_mode = "default"

[paths]
base_output_dir = "./research-outputs"
checkpoint_dir = "~/.thoth/checkpoints"

[execution]
poll_interval = 30
max_wait = 30

[output]
combine_reports = false
format = "markdown"
include_metadata = true
timestamp_format = "%Y-%m-%d_%H%M%S"

[providers.{test_case.provider}]
api_key = "{api_key}"
"""
                with open("test_config.toml", "w") as f:
                    f.write(config_content)
        
        # Create test query file if needed
        if "--query-file" in test_command:
            idx = test_command.index("--query-file")
            if idx + 1 < len(test_command) and test_command[idx + 1] != "-":
                query_file = test_command[idx + 1]
                if query_file == "test_query.txt":
                    with open(query_file, "w") as f:
                        f.write("test query from file")
        
        # Run the command
        exit_code, stdout, stderr = run_command(
            test_command,
            env=test_env,
            timeout=test_case.timeout,
            signal_after=test_case.signal_after
        )
        
        duration = time.time() - start_time
        
        # Initialize result
        result = TestResult(
            test_id=test_case.test_id,
            passed=True,
            duration=duration,
            stdout=stdout,
            stderr=stderr,
            exit_code=exit_code
        )
        
        # Check if this is an API key failure
        if not result.passed or exit_code != 0:
            result.is_api_key_failure = is_api_key_error(stdout, stderr)
        
        # Show verbose output if enabled
        if self.verbose:
            console.print(f"\n[dim]{'='*60}[/dim]")
            console.print(f"[cyan]Test {test_case.test_id}: {test_case.description}[/cyan]")
            console.print(f"[dim]Command: {' '.join(test_command)}[/dim]")
            console.print(f"[dim]Exit Code: {exit_code}[/dim]")
            if stdout:
                console.print("[dim]--- STDOUT ---[/dim]")
                console.print(stdout)
            if stderr:
                console.print("[dim]--- STDERR ---[/dim]")
                console.print(stderr)
            console.print(f"[dim]{'='*60}[/dim]\n")
        
        # Save output to files if enabled
        if self.save_output:
            test_output_dir = self.output_dir / test_case.test_id
            test_output_dir.mkdir(exist_ok=True)
            
            # Save command info
            with open(test_output_dir / "command.txt", "w") as f:
                f.write(f"Command: {' '.join(test_command)}\n")
                f.write(f"Exit Code: {exit_code}\n")
                f.write(f"Expected Exit Code: {test_case.expected_exit_code}\n")
            
            # Save stdout
            if stdout:
                with open(test_output_dir / "stdout.txt", "w") as f:
                    f.write(stdout)
            
            # Save stderr
            if stderr:
                with open(test_output_dir / "stderr.txt", "w") as f:
                    f.write(stderr)
            
            # Save test metadata
            with open(test_output_dir / "test_info.json", "w") as f:
                json.dump({
                    "test_id": test_case.test_id,
                    "description": test_case.description,
                    "provider": test_case.provider,
                    "passed": False,  # Will be updated later
                    "duration": duration,
                    "timestamp": datetime.now().isoformat()
                }, f, indent=2)
        
        # Validate exit code
        if exit_code != test_case.expected_exit_code:
            result.passed = False
            result.error_message = f"Expected exit code {test_case.expected_exit_code}, got {exit_code}"
            result.is_api_key_failure = is_api_key_error(stdout, stderr)
            return result
        
        # Validate stdout
        passed, error = validate_output(
            stdout, 
            test_case.expected_stdout_patterns,
            test_case.not_expected_stdout_patterns
        )
        if not passed:
            result.passed = False
            result.error_message = f"Stdout validation failed: {error}"
            return result
        
        # Validate stderr
        passed, error = validate_output(
            stderr,
            test_case.expected_stderr_patterns,
            test_case.not_expected_stderr_patterns
        )
        if not passed:
            result.passed = False
            result.error_message = f"Stderr validation failed: {error}"
            return result
        
        # Validate file creation
        if test_case.expected_files:
            passed, created_files, error = validate_file_creation(test_case.expected_files)
            result.created_files = created_files
            if not passed:
                result.passed = False
                result.error_message = f"File validation failed: {error}"
                return result
        
        # Clean up after successful test
        if result.passed and CLEANUP_ENABLED:
            cleanup_test_files(test_case.cleanup_files)
        
        # Update test result in saved output
        if self.save_output:
            test_output_dir = self.output_dir / test_case.test_id
            with open(test_output_dir / "test_info.json", "r") as f:
                test_info = json.load(f)
            test_info["passed"] = result.passed
            test_info["error_message"] = result.error_message
            with open(test_output_dir / "test_info.json", "w") as f:
                json.dump(test_info, f, indent=2)
        
        return result
    
    def run_all_tests(self, test_cases: List[TestCase]):
        """Run all test cases and generate report"""
        total_tests = len(test_cases)
        
        with Progress(
            SpinnerColumn(),
            TextColumn("[progress.description]{task.description}"),
            BarColumn(),
            TextColumn("[progress.percentage]{task.percentage:>3.0f}%"),
            console=console
        ) as progress:
            task = progress.add_task("Running tests...", total=total_tests)
            
            for test_case in test_cases:
                # Update progress
                progress.update(task, description=f"Running {test_case.test_id}: {test_case.description}")
                
                # Run test
                result = self.run_test(test_case)
                self.results.append(result)
                
                # Update progress
                progress.advance(task)
        
        # Generate report
        self.generate_report()
    
    def generate_report(self):
        """Generate and display test report"""
        total_time = time.time() - self.start_time
        passed_tests = sum(1 for r in self.results if r.passed and "Skipped" not in r.stderr)
        skipped_tests = sum(1 for r in self.results if r.passed and "Skipped" in r.stderr)
        failed_tests = sum(1 for r in self.results if not r.passed)
        api_key_failures = sum(1 for r in self.results if not r.passed and r.is_api_key_failure)
        other_failures = failed_tests - api_key_failures
        
        # Summary
        console.print("\n[bold]Test Results Summary[/bold]")
        console.print(f"Total tests: {len(self.results)}")
        console.print(f"Passed: [green]{passed_tests}[/green]")
        console.print(f"Skipped: [yellow]{skipped_tests}[/yellow]")
        if api_key_failures > 0:
            console.print(f"Failed: [red]{failed_tests}[/red] ([yellow]{api_key_failures} due to API key[/yellow], [red]{other_failures} other issues[/red])")
        else:
            console.print(f"Failed: [red]{failed_tests}[/red]")
        console.print(f"Total time: {total_time:.2f}s\n")
        
        # Provider Coverage
        console.print("[bold]Provider Coverage:[/bold]")
        
        # Show what was requested vs what was tested
        if self.requested_providers:
            console.print(f"Requested providers: {', '.join(self.requested_providers)}")
        else:
            console.print("Requested providers: All available")
        
        # Count tests by provider
        provider_test_counts = {"mock": 0, "openai": 0, "perplexity": 0, "none": 0}
        provider_run_counts = {"mock": 0, "openai": 0, "perplexity": 0, "none": 0}
        
        for test_case in all_tests:
            provider_key = test_case.provider or "none"
            if provider_key in provider_test_counts:
                provider_test_counts[provider_key] += 1
        
        for result in self.results:
            test_case = next((tc for tc in all_tests if tc.test_id == result.test_id), None)
            if test_case:
                provider_key = test_case.provider or "none"
                if provider_key in provider_run_counts and not ("Skipped" in result.stderr):
                    provider_run_counts[provider_key] += 1
        
        # Display provider matrix
        console.print("\nProvider Test Matrix:")
        for provider in ["mock", "openai", "perplexity"]:
            total = provider_test_counts[provider]
            ran = provider_run_counts[provider]
            skipped = total - ran
            
            if provider == "mock":
                status = "[green]Available (auto-generated)[/green]"
            elif provider in self.available_providers:
                status = "[green]Available[/green]"
            else:
                status = "[yellow]Not available[/yellow]"
            
            console.print(f"  {provider.capitalize()}: {status} - {ran}/{total} tests run", end="")
            if skipped > 0:
                console.print(f" ({skipped} skipped)")
            else:
                console.print()
        
        # Show provider-agnostic tests
        if provider_test_counts["none"] > 0:
            console.print(f"  Provider-agnostic: {provider_run_counts['none']}/{provider_test_counts['none']} tests run")
        
        console.print("")
        
        # API Key Warning
        if api_key_failures > 0:
            console.print("[yellow]⚠️  API Key Issues Detected[/yellow]")
            console.print(f"{api_key_failures} tests failed due to missing API keys.")
            console.print("Set OPENAI_API_KEY and/or PERPLEXITY_API_KEY environment variables.\n")
        
        # Detailed results table
        table = Table(title="Test Results", show_header=True, header_style="bold magenta")
        table.add_column("Test ID", style="cyan", width=12)
        table.add_column("Description", style="white", width=45)
        table.add_column("Provider", style="white", width=12)
        table.add_column("Status", style="white", width=10)
        table.add_column("Duration", style="white", width=10)
        table.add_column("Error", style="red", width=35)
        
        for result in self.results:
            status = "[green]PASS[/green]" if result.passed else "[red]FAIL[/red]"
            
            # Handle error message and API key indication
            if result.is_api_key_failure:
                error_msg = "🔑 API Key Required"
            else:
                error_msg = result.error_message or ""
                if len(error_msg) > 40:
                    error_msg = error_msg[:37] + "..."
            
            # Find matching test case for description and provider
            test_case = next((tc for tc in all_tests if tc.test_id == result.test_id), None)
            description = test_case.description if test_case else "Unknown test"
            if len(description) > 45:
                description = description[:42] + "..."
            
            # Determine provider info
            if test_case and test_case.provider:
                if result.passed:
                    provider_info = f"[green]{test_case.provider}[/green]"
                else:
                    provider_info = f"[red]{test_case.provider}[/red]"
            else:
                provider_info = "[dim]-[/dim]"
            
            table.add_row(
                result.test_id,
                description,
                provider_info,
                status,
                f"{result.duration:.2f}s",
                error_msg
            )
        
        console.print(table)
        
        # Detailed failure information
        if failed_tests > 0:
            console.print("\n[bold red]Failed Test Details[/bold red]")
            
            # Show API key failures separately
            api_failures = [r for r in self.results if not r.passed and r.is_api_key_failure]
            other_failures = [r for r in self.results if not r.passed and not r.is_api_key_failure]
            
            if api_failures:
                console.print("\n[yellow]Tests Failed Due to Missing API Keys:[/yellow]")
                for result in api_failures:
                    console.print(f"  • {result.test_id}: {result.error_message}")
            
            if other_failures:
                console.print("\n[red]Other Test Failures:[/red]")
                for result in other_failures:
                    console.print(f"\n[yellow]Test {result.test_id}:[/yellow]")
                    console.print(f"Error: {result.error_message}")
                    if result.stdout:
                        # Show more output for debugging (up to 2000 chars)
                        console.print(f"Stdout:\n{result.stdout[:2000]}")
                    if result.stderr:
                        console.print(f"Stderr:\n{result.stderr[:2000]}")
        
        # Exit with appropriate code
        sys.exit(0 if failed_tests == 0 else 1)

# ============================================================================
# Test Definitions
# ============================================================================

# All test cases
all_tests = [
    # Basic CLI Tests (No Provider Required)
    TestCase(
        test_id="M1T-01",
        description="Verify --version displays version",
        command=[THOTH_EXECUTABLE, "--version"],
        expected_stdout_patterns=[r"Thoth v\d+\.\d+\.\d+"],
        expected_exit_code=0
    ),
    
    TestCase(
        test_id="M1T-02",
        description="Quick mode with mock provider creates output file",
        command=[THOTH_EXECUTABLE, "test query", "--provider", "mock"],
        expected_stdout_patterns=[
            r"Research completed",
            r"Results saved to:"
        ],
        expected_files=["*_*_*_mock_*.md"],
        cleanup_files=["*_*_*_mock_*.md"],
        expected_exit_code=0,
        provider="mock",
        api_key_method="env"
    ),
    
    TestCase(
        test_id="M1T-03",
        description="Filename follows correct pattern",
        command=[THOTH_EXECUTABLE, "test filename pattern", "--provider", "mock"],
        expected_stdout_patterns=[r"\d{4}-\d{2}-\d{2}_\d{6}_\w+_mock_[\w-]+\.md"],
        expected_files=["????-??-??_??????_*_mock_*.md"],
        cleanup_files=["*_*_*_mock_*.md"],
        expected_exit_code=0,
        provider="mock",
        api_key_method="env"
    ),
    
    TestCase(
        test_id="M1T-04",
        description="Help text shows quick mode examples",
        command=[THOTH_EXECUTABLE, "--help"],
        expected_stdout_patterns=[
            r"Usage:",
            r"Quick usage:",
            r"thoth \"how does DNS work\""
        ],
        expected_exit_code=0
    ),
    
    # Graceful Shutdown Test (F-81)
    TestCase(
        test_id="M1T-09",
        description="Ctrl-C graceful shutdown with checkpoint save",
        command=[THOTH_EXECUTABLE, "test ctrl-c", "--provider", "mock"],
        expected_exit_code=130,  # Standard exit code for SIGINT
        skip=True,
        skip_reason="Requires manual signal testing"
    ),
    
    TestCase(
        test_id="M1T-05",
        description="Help command works",
        command=[THOTH_EXECUTABLE, "help"],
        expected_stdout_patterns=[
            r"Usage:",
            r"Quick usage:"
        ],
        expected_exit_code=0
    ),
    
    # Mock Provider API Key Tests
    TestCase(
        test_id="MOCK-01",
        description="Mock provider works with environment API key",
        command=[THOTH_EXECUTABLE, "test env key", "--provider", "mock"],
        expected_stdout_patterns=[r"Research completed"],
        expected_files=["*_mock_*.md"],
        cleanup_files=["*_mock_*.md"],
        expected_exit_code=0,
        provider="mock",
        api_key_method="env"
    ),
    
    TestCase(
        test_id="MOCK-02",
        description="Mock provider works with command-line API key",
        command=[THOTH_EXECUTABLE, "test cli key", "--provider", "mock", "--api-key", "dummy-key"],
        expected_stdout_patterns=[r"Research completed"],
        expected_files=["*_mock_*.md"],
        cleanup_files=["*_mock_*.md"],
        expected_exit_code=0,
        provider="mock",
        api_key_method="cli"
    ),
    
    # Provider flag short form test (F-39/T-CLI-03)
    TestCase(
        test_id="T-CLI-03",
        description="Short form -P works for provider flag",
        command=[THOTH_EXECUTABLE, "test short provider flag", "-P", "mock"],
        expected_stdout_patterns=[r"Research completed"],
        expected_files=["*_mock_*.md"],
        cleanup_files=["*_mock_*.md"],
        expected_exit_code=0,
        provider="mock",
        api_key_method="env"
    ),
    
    # Milestone 2: Configuration System
    TestCase(
        test_id="M2T-01",
        description="Environment variables are read (API key error)",
        command=[THOTH_EXECUTABLE, "clarification", "test env"],
        env={"OPENAI_API_KEY": ""},
        expected_stdout_patterns=[r"openai API key not found"],
        expected_exit_code=1
    ),
    
    # API Key Masking Test (F-19/M2-08)
    TestCase(
        test_id="M2T-08",
        description="API keys are masked in all output",
        command=[THOTH_EXECUTABLE, "test masking", "--provider", "mock", "-v"],
        env={"MOCK_API_KEY": "sk-secret-key-12345"},
        not_expected_stdout_patterns=[r"sk-secret-key-12345"],
        expected_stdout_patterns=[r"API Key: sk-\.\.\.345"],
        expected_exit_code=0,
        provider="mock",
        api_key_method="env"
    ),
    
    TestCase(
        test_id="M2T-02",
        description="Init command creates config directory",
        command=[THOTH_EXECUTABLE, "init"],
        env={"XDG_CONFIG_HOME": "./test_temp_config"},
        expected_stdout_patterns=[r"Configuration saved to"],
        expected_exit_code=0,
        cleanup_files=["./test_temp_config/*"]
    ),
    
    # Config flag test (F-40/T-CFG-06)
    TestCase(
        test_id="T-CFG-06",
        description="--config flag loads custom config file",
        command=[THOTH_EXECUTABLE, "test custom config", "--provider", "mock", "--config", "./test_config.toml"],
        expected_stdout_patterns=[r"Research completed"],
        expected_files=["*_mock_*.md"],
        cleanup_files=["*_mock_*.md", "test_config.toml"],
        expected_exit_code=0,
        provider="mock",
        api_key_method="config"
    ),
    
    # Milestone 3: Provider Architecture
    TestCase(
        test_id="M3T-01",
        description="Mock provider completes successfully",
        command=[THOTH_EXECUTABLE, "test mock", "--provider", "mock"],
        expected_stdout_patterns=[r"Research completed"],
        expected_files=["*_mock_*.md"],
        cleanup_files=["*_mock_*.md"],
        expected_exit_code=0,
        provider="mock",
        api_key_method="env"
    ),
    
    TestCase(
        test_id="M3T-02",
        description="--provider flag limits execution",
        command=[THOTH_EXECUTABLE, "deep_research", "test provider", "--provider", "mock"],
        expected_stdout_patterns=[r"Research completed"],
        expected_files=["*_mock_*.md"],
        not_expected_stdout_patterns=[r"_openai_", r"_perplexity_"],
        cleanup_files=["*_mock_*.md"],
        expected_exit_code=0,
        provider="mock",
        api_key_method="env"
    ),
    
    TestCase(
        test_id="M3T-03",
        description="Invalid provider shows error",
        command=[THOTH_EXECUTABLE, "test", "--provider", "invalid-provider"],
        expected_stderr_patterns=[r"Invalid value for '--provider' / '-P': 'invalid-provider' is not one of"],
        expected_exit_code=2
    ),
    
    # Milestone 4: Mode System
    TestCase(
        test_id="M4T-01",
        description="Default mode is 'default' not 'deep_research'",
        command=[THOTH_EXECUTABLE, "test default mode", "--provider", "mock", "-v"],
        expected_stdout_patterns=[r"Operation ID:"],
        expected_files=["*_default_*.md"],
        not_expected_stdout_patterns=[r"Mode: deep_research"],
        cleanup_files=["*_default_*.md"],
        expected_exit_code=0,
        provider="mock",
        api_key_method="env"
    ),
    
    TestCase(
        test_id="M4T-02",
        description="All built-in modes are accessible",
        command=[THOTH_EXECUTABLE, "thinking", "test thinking mode", "--provider", "mock"],
        expected_stdout_patterns=[r"Research completed"],
        expected_files=["*_thinking_*.md"],
        cleanup_files=["*_thinking_*.md"],
        expected_exit_code=0,
        provider="mock",
        api_key_method="env"
    ),
    
    TestCase(
        test_id="M4T-03",
        description="Unknown mode shows error",
        command=[THOTH_EXECUTABLE, "invalid-mode", "test"],
        expected_stdout_patterns=[r"Unknown mode: invalid-mode"],
        expected_exit_code=1
    ),
    
    TestCase(
        test_id="M4T-04",
        description="Clarification mode works",
        command=[THOTH_EXECUTABLE, "clarification", "test clarification", "--provider", "mock"],
        expected_stdout_patterns=[r"Research completed"],
        expected_files=["*_clarification_*.md"],
        cleanup_files=["*_clarification_*.md"],
        expected_exit_code=0,
        provider="mock",
        api_key_method="env"
    ),
    
    TestCase(
        test_id="M4T-05",
        description="Exploration mode works",
        command=[THOTH_EXECUTABLE, "exploration", "test exploration", "--provider", "mock"],
        expected_stdout_patterns=[r"Research completed"],
        expected_files=["*_exploration_*.md"],
        cleanup_files=["*_exploration_*.md"],
        expected_exit_code=0,
        provider="mock",
        api_key_method="env"
    ),
    
    # Milestone 5: Async Operations
    TestCase(
        test_id="M5T-01",
        description="--async returns operation ID immediately",
        command=[THOTH_EXECUTABLE, "test async", "--async", "--provider", "mock"],
        expected_stdout_patterns=[
            r"Operation ID: \w+-\d+-\d+-\w+"
        ],
        expected_exit_code=0,
        timeout=5,  # Should return immediately
        provider="mock",
        api_key_method="env"
    ),
    
    TestCase(
        test_id="M5T-02",
        description="Status command shows help when no ID",
        command=[THOTH_EXECUTABLE, "status"],
        expected_stdout_patterns=[r"Error: status command requires an operation ID"],
        expected_exit_code=1
    ),
    
    TestCase(
        test_id="M5T-03",
        description="List command works",
        command=[THOTH_EXECUTABLE, "list"],
        expected_stdout_patterns=[r"Research Operations|No operations found"],
        expected_exit_code=0
    ),
    
    # Checkpoint Persistence Tests
    TestCase(
        test_id="M5T-04",
        description="Checkpoint saves operation state",
        command=[THOTH_EXECUTABLE, "test checkpoint", "--provider", "mock"],
        expected_stdout_patterns=[r"Research completed"],
        expected_files=["*_mock_*.md"],
        cleanup_files=["*_mock_*.md"],
        expected_exit_code=0,
        provider="mock",
        api_key_method="env"
    ),
    
    TestCase(
        test_id="M5T-05",
        description="Checkpoint recovery on corrupted file",
        command=[THOTH_EXECUTABLE, "status", "invalid-json-checkpoint"],
        expected_stdout_patterns=[r"Operation .* not found|Error loading checkpoint"],
        expected_exit_code=1,
        skip=True,
        skip_reason="Requires creating corrupted checkpoint file"
    ),
    
    # Checkpoint Corruption Recovery Test (F-82)
    TestCase(
        test_id="M5T-09",
        description="Checkpoint corruption detection and recovery",
        command=[THOTH_EXECUTABLE, "status", "corrupted-checkpoint-id"],
        expected_stdout_patterns=[r"Checkpoint corrupted|Invalid checkpoint"],
        expected_exit_code=1,
        skip=True,
        skip_reason="Requires test infrastructure for corrupted checkpoints"
    ),
    
    # Milestone 6: Output Management
    TestCase(
        test_id="M6T-01",
        description="Files created in current directory by default",
        command=[THOTH_EXECUTABLE, "test output", "--provider", "mock"],
        expected_stdout_patterns=[r"Results saved to:"],
        expected_files=["*_mock_*.md"],
        cleanup_files=["*_mock_*.md"],
        expected_exit_code=0,
        provider="mock",
        api_key_method="env"
    ),
    
    TestCase(
        test_id="M6T-02",
        description="Output directory can be specified",
        command=[THOTH_EXECUTABLE, "test output dir", "--provider", "mock", "-o", "test_output"],
        expected_stdout_patterns=[r"Results saved to:"],
        expected_files=["test_output/*_mock_*.md"],
        cleanup_files=["test_output/*"],
        expected_exit_code=0,
        provider="mock",
        api_key_method="env"
    ),
    
    # Milestone 7: Progress and UX
    TestCase(
        test_id="M7T-02",
        description="Verbose mode shows detailed info",
        command=[THOTH_EXECUTABLE, "test verbose", "--provider", "mock", "-v"],
        expected_stdout_patterns=[r"Operation ID:"],
        expected_files=["*_mock_*.md"],
        cleanup_files=["*_mock_*.md"],
        expected_exit_code=0,
        provider="mock",
        api_key_method="env"
    ),
    
    # API Key Tests
    TestCase(
        test_id="API-01",
        description="Missing API key shows helpful error",
        command=[THOTH_EXECUTABLE, "clarification", "test"],
        env={"OPENAI_API_KEY": ""},
        expected_stdout_patterns=[
            r"openai API key not found",
            r"Set OPENAI_API_KEY"
        ],
        expected_exit_code=1
    ),
    
    TestCase(
        test_id="API-02",
        description="Missing mock API key in environment",
        command=[THOTH_EXECUTABLE, "test", "--provider", "mock"],
        env={"MOCK_API_KEY": ""},
        expected_stdout_patterns=[r"mock API key not found"],
        expected_exit_code=1
    ),
    
    TestCase(
        test_id="API-03",
        description="Invalid mock API key format",
        command=[THOTH_EXECUTABLE, "test", "--provider", "mock"],
        env={"MOCK_API_KEY": "invalid"},
        expected_stdout_patterns=[r"Invalid mock API key format"],
        expected_exit_code=1
    ),
    
    # Additional Edge Cases
    TestCase(
        test_id="EDGE-01",
        description="Empty query shows error",
        command=[THOTH_EXECUTABLE, ""],
        expected_stderr_patterns=[r"Query cannot be empty"],
        expected_exit_code=2
    ),
    
    TestCase(
        test_id="EDGE-02",
        description="Very long query is handled",
        command=[THOTH_EXECUTABLE, "a" * 1000, "--provider", "mock"],
        expected_stdout_patterns=[r"Research completed"],
        expected_files=["*_mock_*.md"],
        cleanup_files=["*_mock_*.md"],
        expected_exit_code=0,
        provider="mock",
        api_key_method="env"
    ),
    
    # Combined Report Tests
    TestCase(
        test_id="COMB-01",
        description="Combined report generation with --combined flag",
        command=[THOTH_EXECUTABLE, "test combined", "--combined"],
        expected_stdout_patterns=[r"Research completed"],
        expected_files=["*_mock_*.md", "*_openai_*.md", "*_perplexity_*.md", "*_combined_*.md"],
        cleanup_files=["*_*.md"],
        expected_exit_code=0,
        skip=True,
        skip_reason="Requires multiple providers to be configured"
    ),
    
    TestCase(
        test_id="COMB-02",
        description="No combined report without --combined flag",
        command=[THOTH_EXECUTABLE, "test no combined", "--provider", "mock"],
        expected_stdout_patterns=[r"Research completed"],
        expected_files=["*_mock_*.md"],
        not_expected_stdout_patterns=[r"_combined_"],
        cleanup_files=["*_mock_*.md"],
        expected_exit_code=0,
        provider="mock",
        api_key_method="env"
    ),
    
    # Query File Tests
    TestCase(
        test_id="QFILE-01",
        description="Read query from file",
        command=[THOTH_EXECUTABLE, "--query-file", "test_query.txt", "--provider", "mock"],
        expected_stdout_patterns=[r"Research completed"],
        expected_files=["*_mock_*.md"],
        cleanup_files=["*_mock_*.md", "test_query.txt"],
        expected_exit_code=0,
        provider="mock",
        api_key_method="env"
    ),
    
    TestCase(
        test_id="QFILE-02",
        description="Read query from stdin",
        command=["echo", "test query from stdin", "|", THOTH_EXECUTABLE, "--query-file", "-", "--provider", "mock"],
        expected_stdout_patterns=[r"Research completed"],
        expected_files=["*_mock_*.md"],
        cleanup_files=["*_mock_*.md"],
        expected_exit_code=0,
        provider="mock",
        api_key_method="env",
        skip=True,
        skip_reason="Stdin piping requires shell execution"
    ),
    
    # Config File Tests
    TestCase(
        test_id="CFG-01",
        description="Custom config file with --config",
        command=[THOTH_EXECUTABLE, "test config", "--config", "custom_config.toml", "--provider", "mock"],
        expected_stdout_patterns=[r"Research completed"],
        expected_files=["*_mock_*.md"],
        cleanup_files=["*_mock_*.md", "custom_config.toml"],
        expected_exit_code=0,
        provider="mock",
        api_key_method="env"
    ),
    
    # Quiet Mode Tests
    TestCase(
        test_id="QUIET-01",
        description="Quiet mode suppresses output",
        command=[THOTH_EXECUTABLE, "test quiet", "--quiet", "--provider", "mock"],
        not_expected_stdout_patterns=[r"Progress:", r"Researching:"],
        expected_files=["*_mock_*.md"],
        cleanup_files=["*_mock_*.md"],
        expected_exit_code=0,
        provider="mock",
        api_key_method="env"
    ),
    
    TestCase(
        test_id="QUIET-02",
        description="Quiet mode short form -Q",
        command=[THOTH_EXECUTABLE, "test quiet short", "-Q", "--provider", "mock"],
        not_expected_stdout_patterns=[r"Progress:", r"Researching:"],
        expected_files=["*_mock_*.md"],
        cleanup_files=["*_mock_*.md"],
        expected_exit_code=0,
        provider="mock",
        api_key_method="env"
    ),
    
    # Project Mode Tests
    TestCase(
        test_id="PROJ-01",
        description="Project mode creates subdirectory structure",
        command=[THOTH_EXECUTABLE, "test project", "--project", "test_project", "--provider", "mock"],
        expected_stdout_patterns=[r"Research completed"],
        expected_files=["research-outputs/test_project/*_mock_*.md"],
        cleanup_files=["research-outputs/test_project/*"],
        expected_exit_code=0,
        provider="mock",
        api_key_method="env"
    ),
    
    # Exit Code Tests
    TestCase(
        test_id="EXIT-01",
        description="Missing required argument returns exit code 2",
        command=[THOTH_EXECUTABLE, "--mode"],
        expected_stderr_patterns=[r"Error: Option '--mode' requires an argument"],
        expected_exit_code=2
    ),
    
    TestCase(
        test_id="EXIT-02",
        description="Invalid command returns exit code 2",
        command=[THOTH_EXECUTABLE, "--invalid-flag"],
        expected_stderr_patterns=[r"Error: No such option"],
        expected_exit_code=2
    ),
    
    TestCase(
        test_id="EXIT-03",
        description="General error returns exit code 1",
        command=[THOTH_EXECUTABLE, "test", "--provider", "openai"],
        env={"OPENAI_API_KEY": ""},
        expected_stdout_patterns=[r"openai API key not found"],
        expected_exit_code=1
    ),
    
    # Real Provider Tests (require actual API keys)
    TestCase(
        test_id="REAL-01",
        description="OpenAI provider with real API key",
        command=[THOTH_EXECUTABLE, "test openai", "--provider", "openai"],
        expected_stdout_patterns=[r"Research completed"],
        expected_files=["*_openai_*.md"],
        cleanup_files=["*_openai_*.md"],
        expected_exit_code=0,
        provider="openai",
        api_key_method="env"
    ),
    
    TestCase(
        test_id="REAL-02",
        description="Perplexity provider with real API key",
        command=[THOTH_EXECUTABLE, "test perplexity", "--provider", "perplexity"],
        expected_stdout_patterns=[r"Research completed"],
        expected_files=["*_perplexity_*.md"],
        cleanup_files=["*_perplexity_*.md"],
        expected_exit_code=0,
        provider="perplexity",
        api_key_method="env"
    ),
    
    # Signal handling tests
    TestCase(
        test_id="T-SIG-01",
        description="Graceful exit on Ctrl-C with checkpoint save",
        command=[THOTH_EXECUTABLE, "test interrupt handling", "--provider", "mock"],
        expected_stdout_patterns=[
            r"Interrupt received\. Saving checkpoint",
            r"Checkpoint saved\. Resume with: thoth --resume"
        ],
        expected_exit_code=1,
        provider="mock",
        api_key_method="env",
        signal_after=0.05,  # Send SIGINT after 0.05 seconds
        cleanup_files=["*_mock_*.md", "~/.thoth/checkpoints/*.json"],
        skip=True,
        skip_reason="Timing issues make this test unreliable"
    ),
    
    # Provider discovery tests
    TestCase(
        test_id="T-PROV-06",
        description="Providers command shows usage without --models flag",
        command=[THOTH_EXECUTABLE, "providers"],
        expected_stdout_patterns=[
            r"Usage:.*thoth providers -- --models",
            r"Show available models from each provider"
        ],
        expected_exit_code=0
    ),
    
    TestCase(
        test_id="T-PROV-07",
        description="List mock provider models",
        command=[THOTH_EXECUTABLE, "providers", "--", "--models", "--provider", "mock"],
        expected_stdout_patterns=[
            r"Mock Models",
            r"mock-model-v1",
            r"mock-model-v2"
        ],
        expected_exit_code=0,
        provider="mock",
        api_key_method="env"
    ),
    
    TestCase(
        test_id="T-PROV-08",
        description="List all available models (mock only when no API keys)",
        command=[THOTH_EXECUTABLE, "providers", "--", "--models"],
        expected_stdout_patterns=[
            r"Fetching available models",
            r"Mock Models"
        ],
        expected_exit_code=0,
        provider="mock",
        api_key_method="env"
    ),
    
    TestCase(
        test_id="T-PROV-09",
        description="Help for providers command",
        command=[THOTH_EXECUTABLE, "help", "providers"],
        expected_stdout_patterns=[
            r"thoth providers.*List available models",
            r"OpenAI models are fetched dynamically",
            r"--models.*List available models",
            r"--provider.*Filter by specific provider"
        ],
        expected_exit_code=0
    ),
    
    TestCase(
        test_id="T-PROV-10",
        description="Invalid provider name shows error",
        command=[THOTH_EXECUTABLE, "providers", "--", "--models", "--provider", "invalid"],
        expected_stderr_patterns=[
            r"Unknown provider: invalid",
            r"Available providers: openai, perplexity, mock"
        ],
        expected_exit_code=1
    ),
]

# ============================================================================
# Main Test Execution
# ============================================================================

@click.command()
@click.option('--run', '-r', is_flag=True, help='Run tests (required to execute tests)')
@click.option('--all', '-a', is_flag=True, help='Run all tests with all providers (shorthand for -r --all-providers)')
@click.option('--verbose', '-v', is_flag=True, help='Show full output for each test')
@click.option('--test', '-t', help='Run only tests matching this pattern')
@click.option('--provider', '-p', help='Run tests for specific provider(s), comma-separated (e.g., mock,openai)')
@click.option('--all-providers', is_flag=True, help='Run tests for all providers (requires all API keys)')
@click.option('--no-cleanup', is_flag=True, help='Do not clean up test files')
@click.option('--save-output', is_flag=True, help='Save test output to files for debugging')
def main(run, all, verbose, test, provider, all_providers, no_cleanup, save_output):
    """Thoth Test Suite - Comprehensive regression tests for Thoth CLI
    
    \b
    Examples:
      # Show help (default when no options given)
      ./thoth_test
      
      # Run all tests with all providers (requires all API keys)
      ./thoth_test -a
      
      # Run all tests with available providers
      ./thoth_test -r
      
      # Run only mock provider tests (no API keys needed)
      ./thoth_test -r --provider mock
      
      # Run tests for specific provider (requires API key)
      ./thoth_test -r --provider openai
      
      # Run tests for multiple providers
      ./thoth_test -r --provider mock,perplexity
      
      # Run all provider tests (long form)
      ./thoth_test -r --all-providers
      
      # Run specific test with mock provider
      ./thoth_test -r --provider mock -t M4T-01
      
      # Run tests matching pattern with verbose output
      ./thoth_test -r -t "async" -v
      
      # Save test outputs for debugging
      ./thoth_test -r --provider mock --save-output
    
    \b
    Provider Test Matrix:
      - mock: Always available (auto-generates API key)
      - openai: Requires OPENAI_API_KEY environment variable
      - perplexity: Requires PERPLEXITY_API_KEY environment variable
    
    \b
    Notes:
      - Use -r or --run to execute tests (otherwise shows help)
      - Use -a as shorthand for -r --all-providers
      - When using --provider, only the specified providers' API keys are required
      - When using --all-providers, all API keys must be set
      - Default -r behavior runs tests for all available providers
    """
    global CLEANUP_ENABLED
    
    # Handle -a shorthand
    if all:
        run = True
        all_providers = True
    
    # If no run flag, show help and exit
    if not run:
        ctx = click.get_current_context()
        click.echo(ctx.get_help())
        ctx.exit()
    
    if no_cleanup:
        CLEANUP_ENABLED = False
    
    console.print("[bold cyan]Thoth Test Suite[/bold cyan]")
    console.print(f"Testing executable: {THOTH_EXECUTABLE}")
    
    # Parse provider selection
    requested_providers = None
    if provider and all_providers:
        console.print("[red]Error: Cannot specify both --provider and --all-providers[/red]")
        sys.exit(1)
    
    if provider:
        requested_providers = [p.strip() for p in provider.split(',')]
        # Validate provider names
        valid_providers = ["mock", "openai", "perplexity"]
        invalid = [p for p in requested_providers if p not in valid_providers]
        if invalid:
            console.print(f"[red]Error: Invalid provider(s): {', '.join(invalid)}[/red]")
            console.print(f"Valid providers: {', '.join(valid_providers)}")
            sys.exit(1)
    elif all_providers:
        requested_providers = ["mock", "openai", "perplexity"]
    
    # Filter tests if pattern provided
    tests_to_run = all_tests
    if test:
        tests_to_run = [t for t in all_tests if test in t.test_id or test.lower() in t.description.lower()]
        console.print(f"Running {len(tests_to_run)} tests matching '{test}'")
    else:
        console.print(f"Total tests: {len(tests_to_run)}")
    
    # Filter by provider if requested
    if requested_providers:
        provider_filtered = []
        provider_counts = {p: 0 for p in requested_providers}
        provider_counts["none"] = 0
        
        for t in tests_to_run:
            # Include tests with no provider (provider-agnostic tests)
            if not t.provider:
                provider_filtered.append(t)
                provider_counts["none"] += 1
            # Include tests for requested providers
            elif t.provider in requested_providers:
                provider_filtered.append(t)
                provider_counts[t.provider] += 1
        
        tests_to_run = provider_filtered
        console.print(f"Filtered to {len(tests_to_run)} tests for provider(s): {', '.join(requested_providers)}")
        
        # Show breakdown
        breakdown = []
        for p in requested_providers:
            if provider_counts[p] > 0:
                breakdown.append(f"{p}: {provider_counts[p]}")
        if provider_counts["none"] > 0:
            breakdown.append(f"provider-agnostic: {provider_counts['none']}")
        if breakdown:
            console.print(f"Test breakdown: {', '.join(breakdown)}")
    
    if verbose:
        console.print("[yellow]Verbose mode enabled - showing full output for each test[/yellow]")
    
    # Verify executable exists
    if not Path(THOTH_EXECUTABLE).exists():
        console.print(f"[red]Error: Executable '{THOTH_EXECUTABLE}' not found[/red]")
        sys.exit(1)
    
    # Validate test environment
    test_api_keys = get_test_api_keys()
    validate_test_environment(test_api_keys, requested_providers)
    
    # Create test runner and execute tests
    runner = TestRunner(verbose=verbose, save_output=save_output, requested_providers=requested_providers)
    runner.run_all_tests(tests_to_run)
    
    if save_output and runner.output_dir:
        console.print(f"\n[green]Test outputs saved to: {runner.output_dir}[/green]")

if __name__ == "__main__":
    main()