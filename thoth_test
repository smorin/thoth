#!/usr/bin/env -S uv run --script
# /// script
# requires-python = ">=3.11"
# dependencies = ["rich>=13.7", "click>=8.0"]
# ///
"""
Thoth Test Suite - Comprehensive regression tests for Thoth CLI

This test suite validates all aspects of Thoth's functionality through
black-box testing using subprocess. Each test validates:
- Exit codes
- Standard output patterns
- Standard error patterns  
- File creation and content
"""

import subprocess
import json
import os
import sys
import tempfile
import time
import signal
from dataclasses import dataclass, field
from pathlib import Path
from typing import List, Optional, Dict, Any, Tuple, Union
import re
import shutil
from datetime import datetime

from rich.console import Console
from rich.table import Table
from rich.progress import Progress, SpinnerColumn, TextColumn, BarColumn

# Initialize console for output
console = Console()

# Test configuration
THOTH_EXECUTABLE = "./thoth"
TEST_TIMEOUT = 30
CLEANUP_ENABLED = True

# ============================================================================
# Data Models
# ============================================================================

@dataclass
class TestResult:
    """Result of a single test execution"""
    test_id: str
    passed: bool
    duration: float
    stdout: str
    stderr: str
    exit_code: int
    error_message: Optional[str] = None
    created_files: List[Path] = field(default_factory=list)
    is_api_key_failure: bool = False

@dataclass
class TestCase:
    """Definition of a test case"""
    test_id: str
    description: str
    command: List[str]
    env: Dict[str, str] = field(default_factory=dict)
    expected_exit_code: int = 0
    expected_stdout_patterns: List[str] = field(default_factory=list)
    expected_stderr_patterns: List[str] = field(default_factory=list)
    not_expected_stdout_patterns: List[str] = field(default_factory=list)
    not_expected_stderr_patterns: List[str] = field(default_factory=list)
    expected_files: List[str] = field(default_factory=list)
    timeout: int = TEST_TIMEOUT
    cleanup_files: List[str] = field(default_factory=list)
    skip: bool = False
    skip_reason: str = ""

# ============================================================================
# Utility Functions
# ============================================================================

def run_command(command: List[str], env: Optional[Dict[str, str]] = None, 
                timeout: int = TEST_TIMEOUT, cwd: Optional[str] = None) -> Tuple[int, str, str]:
    """
    Execute a command and return exit code, stdout, stderr
    """
    # Merge environment variables
    cmd_env = os.environ.copy()
    if env:
        cmd_env.update(env)
    
    try:
        result = subprocess.run(
            command,
            capture_output=True,
            text=True,
            timeout=timeout,
            env=cmd_env,
            cwd=cwd
        )
        return result.returncode, result.stdout, result.stderr
    except subprocess.TimeoutExpired:
        return -1, "", f"Command timed out after {timeout} seconds"
    except Exception as e:
        return -1, "", f"Command failed: {str(e)}"

def cleanup_test_files(patterns: List[str]):
    """Remove test artifacts matching patterns"""
    if not CLEANUP_ENABLED:
        return
    
    for pattern in patterns:
        for file_path in Path(".").glob(pattern):
            try:
                if file_path.is_file():
                    file_path.unlink()
                elif file_path.is_dir():
                    shutil.rmtree(file_path)
            except Exception:
                pass

def validate_file_creation(expected_files: List[str]) -> Tuple[bool, List[Path], str]:
    """Check if expected files were created"""
    created_files = []
    missing_files = []
    
    for file_pattern in expected_files:
        matching_files = list(Path(".").glob(file_pattern))
        if matching_files:
            created_files.extend(matching_files)
        else:
            missing_files.append(file_pattern)
    
    if missing_files:
        return False, created_files, f"Missing expected files: {', '.join(missing_files)}"
    
    return True, created_files, ""

def validate_output(output: str, expected_patterns: List[str], 
                   not_expected_patterns: List[str]) -> Tuple[bool, str]:
    """Validate output against expected and not-expected patterns"""
    # Check expected patterns
    for pattern in expected_patterns:
        if not re.search(pattern, output, re.MULTILINE | re.DOTALL):
            return False, f"Pattern not found: {pattern}"
    
    # Check not-expected patterns
    for pattern in not_expected_patterns:
        if re.search(pattern, output, re.MULTILINE | re.DOTALL):
            return False, f"Unexpected pattern found: {pattern}"
    
    return True, ""

def is_api_key_error(stdout: str, stderr: str) -> bool:
    """Check if the error is due to missing API key"""
    error_patterns = [
        r"API key not found",
        r"api key not found",
        r"Set.*API_KEY",
        r"OPENAI_API_KEY",
        r"PERPLEXITY_API_KEY"
    ]
    combined_output = stdout + stderr
    return any(re.search(pattern, combined_output, re.IGNORECASE) for pattern in error_patterns)

def create_test_config(config_dir: Path, content: str):
    """Create a test configuration file"""
    config_dir.mkdir(parents=True, exist_ok=True)
    config_file = config_dir / "config.toml"
    config_file.write_text(content)
    return config_file

def find_latest_output_file(pattern: str = "*_*_*_*_*.md") -> Optional[Path]:
    """Find the most recently created output file matching pattern"""
    files = sorted(Path(".").glob(pattern), key=lambda p: p.stat().st_mtime, reverse=True)
    return files[0] if files else None

# ============================================================================
# Test Runner
# ============================================================================

class TestRunner:
    """Executes test cases and validates results"""
    
    def __init__(self):
        self.results: List[TestResult] = []
        self.start_time = time.time()
    
    def run_test(self, test_case: TestCase) -> TestResult:
        """Execute a single test case"""
        if test_case.skip:
            return TestResult(
                test_id=test_case.test_id,
                passed=True,
                duration=0,
                stdout="",
                stderr=f"Skipped: {test_case.skip_reason}",
                exit_code=0
            )
        
        start_time = time.time()
        
        # Clean up any existing test files
        cleanup_test_files(test_case.cleanup_files)
        
        # Run the command
        exit_code, stdout, stderr = run_command(
            test_case.command,
            env=test_case.env,
            timeout=test_case.timeout
        )
        
        duration = time.time() - start_time
        
        # Initialize result
        result = TestResult(
            test_id=test_case.test_id,
            passed=True,
            duration=duration,
            stdout=stdout,
            stderr=stderr,
            exit_code=exit_code
        )
        
        # Check if this is an API key failure
        if not result.passed or exit_code != 0:
            result.is_api_key_failure = is_api_key_error(stdout, stderr)
        
        # Validate exit code
        if exit_code != test_case.expected_exit_code:
            result.passed = False
            result.error_message = f"Expected exit code {test_case.expected_exit_code}, got {exit_code}"
            result.is_api_key_failure = is_api_key_error(stdout, stderr)
            return result
        
        # Validate stdout
        passed, error = validate_output(
            stdout, 
            test_case.expected_stdout_patterns,
            test_case.not_expected_stdout_patterns
        )
        if not passed:
            result.passed = False
            result.error_message = f"Stdout validation failed: {error}"
            return result
        
        # Validate stderr
        passed, error = validate_output(
            stderr,
            test_case.expected_stderr_patterns,
            test_case.not_expected_stderr_patterns
        )
        if not passed:
            result.passed = False
            result.error_message = f"Stderr validation failed: {error}"
            return result
        
        # Validate file creation
        if test_case.expected_files:
            passed, created_files, error = validate_file_creation(test_case.expected_files)
            result.created_files = created_files
            if not passed:
                result.passed = False
                result.error_message = f"File validation failed: {error}"
                return result
        
        # Clean up after successful test
        if result.passed and CLEANUP_ENABLED:
            cleanup_test_files(test_case.cleanup_files)
        
        return result
    
    def run_all_tests(self, test_cases: List[TestCase]):
        """Run all test cases and generate report"""
        total_tests = len(test_cases)
        
        with Progress(
            SpinnerColumn(),
            TextColumn("[progress.description]{task.description}"),
            BarColumn(),
            TextColumn("[progress.percentage]{task.percentage:>3.0f}%"),
            console=console
        ) as progress:
            task = progress.add_task("Running tests...", total=total_tests)
            
            for test_case in test_cases:
                # Update progress
                progress.update(task, description=f"Running {test_case.test_id}: {test_case.description}")
                
                # Run test
                result = self.run_test(test_case)
                self.results.append(result)
                
                # Update progress
                progress.advance(task)
        
        # Generate report
        self.generate_report()
    
    def generate_report(self):
        """Generate and display test report"""
        total_time = time.time() - self.start_time
        passed_tests = sum(1 for r in self.results if r.passed)
        failed_tests = len(self.results) - passed_tests
        api_key_failures = sum(1 for r in self.results if not r.passed and r.is_api_key_failure)
        other_failures = failed_tests - api_key_failures
        
        # Summary
        console.print("\n[bold]Test Results Summary[/bold]")
        console.print(f"Total tests: {len(self.results)}")
        console.print(f"Passed: [green]{passed_tests}[/green]")
        if api_key_failures > 0:
            console.print(f"Failed: [red]{failed_tests}[/red] ([yellow]{api_key_failures} due to API key[/yellow], [red]{other_failures} other issues[/red])")
        else:
            console.print(f"Failed: [red]{failed_tests}[/red]")
        console.print(f"Total time: {total_time:.2f}s\n")
        
        # API Key Warning
        if api_key_failures > 0:
            console.print("[yellow]⚠️  API Key Issues Detected[/yellow]")
            console.print(f"{api_key_failures} tests failed due to missing API keys.")
            console.print("Set OPENAI_API_KEY and/or PERPLEXITY_API_KEY environment variables.\n")
        
        # Detailed results table
        table = Table(title="Test Results", show_header=True, header_style="bold magenta")
        table.add_column("Test ID", style="cyan", width=12)
        table.add_column("Description", style="white", width=50)
        table.add_column("Status", style="white", width=10)
        table.add_column("Duration", style="white", width=10)
        table.add_column("Error", style="red", width=40)
        
        for result in self.results:
            status = "[green]PASS[/green]" if result.passed else "[red]FAIL[/red]"
            
            # Handle error message and API key indication
            if result.is_api_key_failure:
                error_msg = "🔑 API Key Required"
            else:
                error_msg = result.error_message or ""
                if len(error_msg) > 40:
                    error_msg = error_msg[:37] + "..."
            
            # Find matching test case for description
            test_case = next((tc for tc in all_tests if tc.test_id == result.test_id), None)
            description = test_case.description if test_case else "Unknown test"
            if len(description) > 50:
                description = description[:47] + "..."
            
            table.add_row(
                result.test_id,
                description,
                status,
                f"{result.duration:.2f}s",
                error_msg
            )
        
        console.print(table)
        
        # Detailed failure information
        if failed_tests > 0:
            console.print("\n[bold red]Failed Test Details[/bold red]")
            
            # Show API key failures separately
            api_failures = [r for r in self.results if not r.passed and r.is_api_key_failure]
            other_failures = [r for r in self.results if not r.passed and not r.is_api_key_failure]
            
            if api_failures:
                console.print("\n[yellow]Tests Failed Due to Missing API Keys:[/yellow]")
                for result in api_failures:
                    console.print(f"  • {result.test_id}: {result.error_message}")
            
            if other_failures:
                console.print("\n[red]Other Test Failures:[/red]")
                for result in other_failures:
                    console.print(f"\n[yellow]Test {result.test_id}:[/yellow]")
                    console.print(f"Error: {result.error_message}")
                    if result.stdout:
                        console.print(f"Stdout:\n{result.stdout[:500]}")
                    if result.stderr:
                        console.print(f"Stderr:\n{result.stderr[:500]}")
        
        # Exit with appropriate code
        sys.exit(0 if failed_tests == 0 else 1)

# ============================================================================
# Test Definitions
# ============================================================================

# All test cases
all_tests = [
    # Milestone 1: Core CLI Foundation
    TestCase(
        test_id="M1T-01",
        description="Verify --version displays version",
        command=[THOTH_EXECUTABLE, "--version"],
        expected_stdout_patterns=[r"Thoth v\d+\.\d+\.\d+"],
        expected_exit_code=0
    ),
    
    TestCase(
        test_id="M1T-02",
        description="Quick mode with mock provider creates output file",
        command=[THOTH_EXECUTABLE, "test query", "--provider", "mock"],
        expected_stdout_patterns=[
            r"Research completed",
            r"Files created:"
        ],
        expected_files=["*_*_*_mock_*.md"],
        cleanup_files=["*_*_*_mock_*.md"],
        expected_exit_code=0
    ),
    
    TestCase(
        test_id="M1T-03",
        description="Filename follows correct pattern",
        command=[THOTH_EXECUTABLE, "test filename pattern", "--provider", "mock"],
        expected_stdout_patterns=[r"\d{4}-\d{2}-\d{2}_\d{6}_\w+_mock_[\w-]+\.md"],
        expected_files=["????-??-??_??????_*_mock_*.md"],
        cleanup_files=["*_*_*_mock_*.md"],
        expected_exit_code=0
    ),
    
    TestCase(
        test_id="M1T-04",
        description="Help text shows quick mode examples",
        command=[THOTH_EXECUTABLE, "--help"],
        expected_stdout_patterns=[
            r"Usage:",
            r"Quick usage:",
            r"thoth \"how does DNS work\""
        ],
        expected_exit_code=0
    ),
    
    TestCase(
        test_id="M1T-05",
        description="Help command works",
        command=[THOTH_EXECUTABLE, "help"],
        expected_stdout_patterns=[
            r"Usage:",
            r"Quick usage:"
        ],
        expected_exit_code=0
    ),
    
    # Milestone 2: Configuration System
    TestCase(
        test_id="M2T-01",
        description="Environment variables are read (API key error)",
        command=[THOTH_EXECUTABLE, "clarification", "test env"],
        env={"OPENAI_API_KEY": ""},
        expected_stderr_patterns=[r"openai API key not found"],
        expected_exit_code=1
    ),
    
    TestCase(
        test_id="M2T-02",
        description="Init command creates config directory",
        command=[THOTH_EXECUTABLE, "init"],
        expected_stdout_patterns=[r"Configuration saved to"],
        expected_exit_code=0,
        skip=True,
        skip_reason="Would modify user config"
    ),
    
    # Milestone 3: Provider Architecture
    TestCase(
        test_id="M3T-01",
        description="Mock provider completes successfully",
        command=[THOTH_EXECUTABLE, "test mock", "--provider", "mock"],
        expected_stdout_patterns=[r"Research completed"],
        expected_files=["*_mock_*.md"],
        cleanup_files=["*_mock_*.md"],
        expected_exit_code=0
    ),
    
    TestCase(
        test_id="M3T-02",
        description="--provider flag limits execution",
        command=[THOTH_EXECUTABLE, "deep_research", "test provider", "--provider", "mock"],
        expected_stdout_patterns=[r"Research completed"],
        expected_files=["*_mock_*.md"],
        not_expected_stdout_patterns=[r"_openai_", r"_perplexity_"],
        cleanup_files=["*_mock_*.md"],
        expected_exit_code=0
    ),
    
    TestCase(
        test_id="M3T-03",
        description="Invalid provider shows error",
        command=[THOTH_EXECUTABLE, "test", "--provider", "invalid-provider"],
        expected_stderr_patterns=[r"Unknown provider: invalid-provider"],
        expected_exit_code=1
    ),
    
    # Milestone 4: Mode System
    TestCase(
        test_id="M4T-01",
        description="Default mode is 'default' not 'deep_research'",
        command=[THOTH_EXECUTABLE, "test default mode", "--provider", "mock", "-v"],
        expected_stdout_patterns=[r"Mode: default"],
        expected_files=["*_default_*.md"],
        not_expected_stdout_patterns=[r"Mode: deep_research"],
        cleanup_files=["*_default_*.md"],
        expected_exit_code=0
    ),
    
    TestCase(
        test_id="M4T-02",
        description="All built-in modes are accessible",
        command=[THOTH_EXECUTABLE, "thinking", "test thinking mode", "--provider", "mock"],
        expected_stdout_patterns=[r"Research completed"],
        expected_files=["*_thinking_*.md"],
        cleanup_files=["*_thinking_*.md"],
        expected_exit_code=0
    ),
    
    TestCase(
        test_id="M4T-03",
        description="Unknown mode shows error",
        command=[THOTH_EXECUTABLE, "invalid-mode", "test"],
        expected_stderr_patterns=[r"Unknown mode: invalid-mode"],
        expected_exit_code=1
    ),
    
    TestCase(
        test_id="M4T-04",
        description="Clarification mode works",
        command=[THOTH_EXECUTABLE, "clarification", "test clarification", "--provider", "mock"],
        expected_stdout_patterns=[r"Research completed"],
        expected_files=["*_clarification_*.md"],
        cleanup_files=["*_clarification_*.md"],
        expected_exit_code=0
    ),
    
    TestCase(
        test_id="M4T-05",
        description="Exploration mode works",
        command=[THOTH_EXECUTABLE, "exploration", "test exploration", "--provider", "mock"],
        expected_stdout_patterns=[r"Research completed"],
        expected_files=["*_exploration_*.md"],
        cleanup_files=["*_exploration_*.md"],
        expected_exit_code=0
    ),
    
    # Milestone 5: Async Operations
    TestCase(
        test_id="M5T-01",
        description="--async returns operation ID immediately",
        command=[THOTH_EXECUTABLE, "test async", "--async", "--provider", "mock"],
        expected_stdout_patterns=[
            r"Research submitted",
            r"Operation ID: \w+-\d+-\d+-\w+"
        ],
        expected_exit_code=0,
        timeout=5  # Should return immediately
    ),
    
    TestCase(
        test_id="M5T-02",
        description="Status command shows help when no ID",
        command=[THOTH_EXECUTABLE, "status"],
        expected_stderr_patterns=[r"operation_id"],
        expected_exit_code=2
    ),
    
    TestCase(
        test_id="M5T-03",
        description="List command works",
        command=[THOTH_EXECUTABLE, "list"],
        expected_stdout_patterns=[r"Recent Operations|No recent operations"],
        expected_exit_code=0
    ),
    
    # Milestone 6: Output Management
    TestCase(
        test_id="M6T-01",
        description="Files created in current directory by default",
        command=[THOTH_EXECUTABLE, "test output", "--provider", "mock"],
        expected_stdout_patterns=[r"Files created:"],
        expected_files=["*_mock_*.md"],
        cleanup_files=["*_mock_*.md"],
        expected_exit_code=0
    ),
    
    TestCase(
        test_id="M6T-02",
        description="Output directory can be specified",
        command=[THOTH_EXECUTABLE, "test output dir", "--provider", "mock", "-o", "test_output"],
        expected_stdout_patterns=[r"Files created:"],
        cleanup_files=["test_output/*"],
        expected_exit_code=0,
        skip=True,
        skip_reason="Output directory feature not fully implemented"
    ),
    
    # Milestone 7: Progress and UX
    TestCase(
        test_id="M7T-01",
        description="Quiet mode suppresses output",
        command=[THOTH_EXECUTABLE, "test quiet", "--provider", "mock", "-q"],
        not_expected_stdout_patterns=[r"Research completed"],
        expected_files=["*_mock_*.md"],
        cleanup_files=["*_mock_*.md"],
        expected_exit_code=0
    ),
    
    TestCase(
        test_id="M7T-02",
        description="Verbose mode shows detailed info",
        command=[THOTH_EXECUTABLE, "test verbose", "--provider", "mock", "-v"],
        expected_stdout_patterns=[r"Operation ID:"],
        expected_files=["*_mock_*.md"],
        cleanup_files=["*_mock_*.md"],
        expected_exit_code=0
    ),
    
    # API Key Tests
    TestCase(
        test_id="API-01",
        description="Missing API key shows helpful error",
        command=[THOTH_EXECUTABLE, "clarification", "test"],
        env={"OPENAI_API_KEY": ""},
        expected_stderr_patterns=[
            r"openai API key not found",
            r"Set OPENAI_API_KEY"
        ],
        expected_exit_code=1
    ),
    
    # Additional Edge Cases
    TestCase(
        test_id="EDGE-01",
        description="Empty query shows error",
        command=[THOTH_EXECUTABLE, ""],
        expected_stderr_patterns=[r"query.*required"],
        expected_exit_code=2
    ),
    
    TestCase(
        test_id="EDGE-02",
        description="Very long query is handled",
        command=[THOTH_EXECUTABLE, "a" * 1000, "--provider", "mock"],
        expected_stdout_patterns=[r"Research completed"],
        expected_files=["*_mock_*.md"],
        cleanup_files=["*_mock_*.md"],
        expected_exit_code=0
    ),
]

# ============================================================================
# Main Test Execution
# ============================================================================

def main():
    """Main test execution"""
    console.print("[bold cyan]Thoth Test Suite[/bold cyan]")
    console.print(f"Testing executable: {THOTH_EXECUTABLE}")
    console.print(f"Total tests: {len(all_tests)}\n")
    
    # Verify executable exists
    if not Path(THOTH_EXECUTABLE).exists():
        console.print(f"[red]Error: Executable '{THOTH_EXECUTABLE}' not found[/red]")
        sys.exit(1)
    
    # Create test runner and execute tests
    runner = TestRunner()
    runner.run_all_tests(all_tests)

if __name__ == "__main__":
    main()