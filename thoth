#!/usr/bin/env -S uv run --script
# /// script
# requires-python = ">=3.11"
# dependencies = [
#   "click>=8.0",
#   "openai>=1.14.0",
#   "httpx>=0.27.0",
#   "rich>=13.7",
#   "platformdirs>=3.0",
#   "aiofiles>=23.0",
#   "tenacity>=8.0",
#   "python-dateutil>=2.8"
# ]
# ///
"""
Thoth - AI-Powered Research Assistant

A command-line tool that automates deep technical research using multiple LLM providers.
"""

import asyncio
import json
import os
import sys
import time
import tomllib
from dataclasses import dataclass, asdict, field
from datetime import datetime, timedelta
from pathlib import Path
from typing import List, Optional, Dict, Any
from uuid import uuid4
import re
import shutil
import signal

import click
from rich.console import Console
from rich.progress import Progress, SpinnerColumn, TextColumn, BarColumn, TimeElapsedColumn
from rich.table import Table
from rich import box
from platformdirs import user_config_dir
from tenacity import retry, stop_after_attempt, wait_exponential, retry_if_exception_type
from openai import AsyncOpenAI
import httpx
import aiofiles

# Initialize console for rich output
console = Console()

# Version tracking
THOTH_VERSION = "1.5.0"
CONFIG_VERSION = "1.0"

# Global variables for signal handling
_current_checkpoint_manager = None
_current_operation = None

# Global config path
_config_path = None

def get_config():
    """Get Config instance with custom path if provided"""
    return Config(_config_path) if _config_path else Config()

# Built-in mode definitions
BUILTIN_MODES = {
    "default": {
        "system_prompt": None,  # No system prompt - pass query directly
        "description": "Default mode - passes query directly to LLM without any system prompt",
        "auto_input": False
    },
    "clarification": {
        "provider": "openai",
        "model": "gpt-4o-mini",
        "system_prompt": """I don't want you to follow the above question and instructions; I want you to tell me the ways this is unclear, point out any ambiguities or anything you don't understand. Follow that by asking questions to help clarify the ambiguous points. Once there are no more unclear, ambiguous or not understood portions, help me draft a clear version of the question/instruction.""",
        "description": "Clarifying takes the prompt to get. Ask clarifying questions to get rid of anything that's ambiguous, unclear, and also make suggestions on what would be a better question.",
        "next": "exploration"
    },
    "exploration": {
        "provider": "openai",
        "model": "gpt-4o",
        "system_prompt": "Explore the topic at hand, looking at options, alternatives, different trade-offs, and make recommendations based on the use case or alternative/related technologies.",
        "description": "Exploration looks at the topic at hand and explores some options and alternatives, different trade-offs, and makes recommendations based on the use case or just alternative and related technologies.",
        "previous": "clarification",
        "next": "deep_dive"
    },
    "deep_dive": {
        "provider": "openai",
        "model": "gpt-4o",
        "system_prompt": "Deep dive into the specific technology, giving an overview, going deep on it, discussing it, and exploring it. For APIs, cover what the API is, how it works, assumptions, dependencies, if it's deprecated, common pitfalls. For other technologies, cover what the technology is and how it's used.",
        "description": "This deep dives into a specific technology, giving an overview of it, going deep on it, discussing it, and exploring it.",
        "previous": "exploration",
        "next": "tutorial"
    },
    "tutorial": {
        "provider": "openai", 
        "model": "gpt-4o",
        "system_prompt": "Create a detailed tutorial with examples of how the technologies are used in common scenarios to get started, along with code samples, command-line execution process, and other useful information.",
        "description": "The tutorial goes into a detailed explanation with examples of how the technologies are used in common scenarios to get started.",
        "previous": "deep_dive",
        "next": "solution"
    },
    "solution": {
        "provider": "openai",
        "model": "gpt-4o",
        "system_prompt": "Design a specific solution to solve the given problem using appropriate technology. Focus on practical implementation.",
        "description": "A solution generally goes into a specific solution to solve a specific problem using technology.",
        "previous": "tutorial",
        "next": "prd"
    },
    "prd": {
        "provider": "openai",
        "model": "gpt-4o",
        "system_prompt": "Create a Product Requirements Document based on prior research. Use previous research on solutions and technologies to create a comprehensive requirements document.",
        "description": "Product Requirements Document based on prior research, we'll create the PRD looking at previous research on solutions to technologies.",
        "previous": "solution",
        "next": "tdd"
    },
    "tdd": {
        "provider": "openai",
        "model": "gpt-4o",
        "system_prompt": "Create a Technical Design Document based on the PRD and prior research. Consider best practices on architecture and good abstractions to make things maintainable and well-structured in code.",
        "description": "The Technical Design Document based on the PRD and prior research puts together a technical design document.",
        "previous": "prd"
    },
    "thinking": {
        "provider": "openai",
        "model": "gpt-4o-mini",
        "temperature": 0.4,
        "system_prompt": "You are a helpful assistant for quick analysis.",
        "description": "Quick thinking and analysis mode for simple questions."
    },
    "deep_research": {
        "providers": ["openai"],
        "parallel": True,
        "system_prompt": "Conduct comprehensive research with citations and multiple perspectives.\nOrganize findings clearly and highlight key insights.",
        "description": "Deep research mode using OpenAI for comprehensive analysis.",
        "previous": "exploration",
        "auto_input": True
    }
}

# ============================================================================
# Configuration Management
# ============================================================================

class Config:
    """Manages configuration loading and validation"""
    
    def __init__(self, config_path: Optional[Path] = None):
        self.config_path = config_path or Path(user_config_dir("thoth")) / "config.toml"
        self.data = self._load_config()
    
    def _load_config(self) -> Dict[str, Any]:
        """Load and validate configuration"""
        if not self.config_path.exists():
            return self._default_config()
        
        with open(self.config_path, "rb") as f:
            config = tomllib.load(f)
        
        # Check configuration version
        config_version = config.get("version", "0.0")
        if config_version != CONFIG_VERSION:
            console.print(f"[yellow]Warning:[/yellow] Configuration version mismatch. Expected {CONFIG_VERSION}, found {config_version}")
            console.print("[yellow]Some settings may not work as expected. Run 'thoth init' to update.[/yellow]")
        
        # Expand paths - handle symlinks by resolving to absolute paths
        if "paths" in config:
            for key, value in config["paths"].items():
                path = Path(value).expanduser()
                if path.exists() and path.is_symlink():
                    path = path.resolve()
                config["paths"][key] = str(path)
        
        # Handle environment variables
        config = self._substitute_env_vars(config)
        
        return config
    
    def _substitute_env_vars(self, config: Dict[str, Any]) -> Dict[str, Any]:
        """Replace ${VAR} with environment variable values"""
        def substitute(value):
            if isinstance(value, str) and value.startswith("${") and value.endswith("}"):
                var_name = value[2:-1]
                return os.getenv(var_name, value)
            elif isinstance(value, dict):
                return {k: substitute(v) for k, v in value.items()}
            elif isinstance(value, list):
                return [substitute(v) for v in value]
            return value
        
        return substitute(config)
    
    def _default_config(self) -> Dict[str, Any]:
        """Return default configuration"""
        return {
            "version": CONFIG_VERSION,
            "general": {
                "default_project": "",  # Empty means ad-hoc mode
                "default_mode": "default"
            },
            "paths": {
                "base_output_dir": "./research-outputs",
                "checkpoint_dir": str(Path(user_config_dir("thoth")) / "checkpoints")
            },
            "execution": {
                "poll_interval": 30,
                "max_wait": 30,
                "parallel_providers": True,
                "retry_attempts": 3,
                "auto_input": True
            },
            "output": {
                "combine_reports": False,
                "format": "markdown",
                "include_metadata": True,
                "timestamp_format": "%Y-%m-%d_%H%M%S"
            },
            "providers": {
                "openai": {
                    "api_key": "${OPENAI_API_KEY}"
                },
                "perplexity": {
                    "api_key": "${PERPLEXITY_API_KEY}"
                }
            },
            "modes": {}  # Modes will be merged with built-in modes
        }
    
    def get_mode_config(self, mode: str) -> Dict[str, Any]:
        """Get mode configuration, merging built-in with user config"""
        # Start with built-in mode if it exists
        mode_config = BUILTIN_MODES.get(mode, {}).copy()
        
        # Override with user config if present
        user_mode = self.data.get("modes", {}).get(mode, {})
        mode_config.update(user_mode)
        
        return mode_config

# ============================================================================
# Utility Functions
# ============================================================================

def generate_operation_id() -> str:
    """Generate unique operation ID with 16-char UUID suffix"""
    timestamp = datetime.now().strftime("%Y%m%d-%H%M%S")
    unique_suffix = str(uuid4()).replace('-', '')[:16]  # 16 chars for better uniqueness
    return f"research-{timestamp}-{unique_suffix}"

def sanitize_slug(text: str, max_length: int = 50) -> str:
    """Convert text to filename-safe slug"""
    # Keep alphanumeric and spaces, replace spaces with hyphens
    slug = re.sub(r'[^a-zA-Z0-9\s-]', '', text)
    slug = re.sub(r'\s+', '-', slug.strip())
    return slug[:max_length].lower()

def mask_api_key(key: str) -> str:
    """Mask API key for display"""
    if not key or len(key) < 8:
        return "***"
    return f"{key[:3]}...{key[-3:]}"

def check_disk_space(path: Path, required_mb: int = 100) -> bool:
    """Check if sufficient disk space is available"""
    stat = shutil.disk_usage(path)
    available_mb = stat.free / (1024 * 1024)
    return available_mb >= required_mb

# ============================================================================
# Data Models
# ============================================================================

@dataclass
class OperationStatus:
    """Status of a research operation"""
    id: str
    query: str
    mode: str
    status: str  # "queued", "running", "completed", "failed", "cancelled"
    created_at: datetime
    updated_at: datetime
    providers: Dict[str, Dict[str, Any]] = field(default_factory=dict)
    output_paths: Dict[str, Path] = field(default_factory=dict)
    error: Optional[str] = None
    progress: float = 0.0  # 0.0 to 1.0
    project: Optional[str] = None
    input_files: List[Path] = field(default_factory=list)

# ============================================================================
# Exception Classes
# ============================================================================

class ThothError(Exception):
    """Base exception for Thoth errors"""
    def __init__(self, message: str, suggestion: str = None, exit_code: int = 1):
        self.message = message
        self.suggestion = suggestion
        self.exit_code = exit_code
        super().__init__(message)

class APIKeyError(ThothError):
    """Missing or invalid API key"""
    def __init__(self, provider: str):
        super().__init__(
            f"{provider} API key not found",
            f"Set {provider.upper()}_API_KEY or run 'thoth init'",
            exit_code=2
        )

class ProviderError(ThothError):
    """Provider-specific error"""
    def __init__(self, provider: str, message: str):
        super().__init__(
            f"{provider} error: {message}",
            "Check API status or try again later",
            exit_code=3
        )

class DiskSpaceError(ThothError):
    """Insufficient disk space"""
    def __init__(self, message: str):
        super().__init__(
            message,
            "Free up disk space and try again",
            exit_code=8
        )

class APIQuotaError(ThothError):
    """API quota exceeded"""
    def __init__(self, provider: str):
        super().__init__(
            f"{provider} API quota exceeded",
            "Wait for quota reset or upgrade your plan",
            exit_code=9
        )

def handle_error(error: Exception):
    """Display error with appropriate formatting"""
    if isinstance(error, ThothError):
        console.print(f"\n[red]Error:[/red] {error.message}")
        if error.suggestion:
            console.print(f"[yellow]Suggestion:[/yellow] {error.suggestion}")
        sys.exit(error.exit_code)
    elif isinstance(error, KeyboardInterrupt):
        console.print("\n[yellow]Operation cancelled by user[/yellow]")
        sys.exit(1)
    else:
        console.print(f"\n[red]Unexpected error:[/red] {str(error)}")
        console.print("[dim]Please report this issue[/dim]")
        if os.getenv("THOTH_DEBUG"):
            console.print_exception()
        sys.exit(127)

# ============================================================================
# Custom Click Command Class
# ============================================================================

class ThothCommand(click.Command):
    """Custom command class to enhance help display"""
    
    def parse_args(self, ctx, args):
        """Override to intercept --help with subcommands"""
        # Check if --help is present
        if '--help' in args:
            help_index = args.index('--help')
            
            # Check if --help is followed by a command
            if help_index + 1 < len(args):
                subcommand = args[help_index + 1]
                
                # Show subcommand help instead of general help
                if subcommand == 'init':
                    show_init_help()
                    ctx.exit(0)
                elif subcommand == 'status':
                    show_status_help()
                    ctx.exit(0)
                elif subcommand == 'list':
                    show_list_help()
                    ctx.exit(0)
                elif subcommand == 'providers':
                    show_providers_help()
                    ctx.exit(0)
        
        return super().parse_args(ctx, args)
    
    def format_epilog(self, ctx, formatter):
        """Override to format epilog without rewrapping"""
        if self.epilog:
            formatter.write_paragraph()
            # Split epilog into lines and write each one to preserve formatting
            for line in self.epilog.split('\n'):
                if line:
                    formatter.write_text(line)
                else:
                    formatter.write_paragraph()

def build_epilog():
    """Build the epilog text with modes and examples"""
    lines = []
    
    # Commands section
    lines.append("Commands:")
    lines.append("  init            Initialize configuration")
    lines.append("  status <ID>     Check operation status") 
    lines.append("  list            List research operations")
    lines.append("  help [COMMAND]  Show help (general or command-specific)")
    lines.append("")
    
    # Research modes section
    lines.append("Research Modes:")
    for mode_name, mode_config in BUILTIN_MODES.items():
        desc = mode_config.get('description', 'No description')
        # Truncate long descriptions to fit nicely
        if len(desc) > 60:
            desc = desc[:57] + "..."
        lines.append(f"  {mode_name:<15} {desc}")
    lines.append("")
    
    # Examples section
    lines.append("Examples:")
    lines.append("  # Simple query (uses default mode)")
    lines.append("  $ thoth \"how does DNS work\"")
    lines.append("")
    lines.append("  # Specify a research mode")
    lines.append("  $ thoth deep_research \"explain kubernetes networking\"")
    lines.append("")
    lines.append("  # Get command-specific help")
    lines.append("  $ thoth help init")
    lines.append("  $ thoth --help status")
    lines.append("")
    lines.append("For detailed command help: thoth help [COMMAND]")
    
    return '\n'.join(lines)


# ============================================================================
# Click CLI Implementation
# ============================================================================

@click.command(
    cls=ThothCommand,
    context_settings=dict(allow_extra_args=True, allow_interspersed_args=True),
    epilog=build_epilog()
)
@click.pass_context
@click.argument('args', nargs=-1)
@click.option('--mode', '-m', 'mode_opt', help='Research mode')
@click.option('--query', '-q', 'query_opt', help='Research query')
@click.option('--query-file', '-F', help='Read query from file (use - for stdin)')
@click.option('--async', '-A', 'async_mode', is_flag=True, help='Submit and exit')
@click.option('--resume', '-R', 'resume_id', help='Resume operation by ID')
@click.option('--project', '-p', help='Project name')
@click.option('--output-dir', '-o', help='Override output directory')
@click.option('--provider', '-P', type=click.Choice(['openai', 'perplexity','mock']), help='Single provider')
@click.option('--input-file', help='Use output from previous mode as input')
@click.option('--auto', is_flag=True, help='Automatically use latest relevant output as input')
@click.option('--verbose', '-v', is_flag=True, help='Enable debug output')
@click.option('--version', '-V', is_flag=True, help='Show version and exit')
@click.option('--api-key-openai', help='API key for OpenAI provider')
@click.option('--api-key-perplexity', help='API key for Perplexity provider')
@click.option('--api-key-mock', help='API key for Mock provider')
@click.option('--config', '-c', 'config_path', help='Path to custom config file')
@click.option('--combined', is_flag=True, help='Generate combined report from multiple providers')
@click.option('--quiet', '-Q', is_flag=True, help='Minimal output during execution')
@click.option('--no-metadata', is_flag=True, help='Disable metadata headers and prompt section in output files')
@click.option('--timeout', '-T', type=float, help='Override request timeout in seconds')
def cli(ctx, args, mode_opt, query_opt, query_file, async_mode, resume_id, 
        project, output_dir, provider, input_file, auto, verbose, version, 
        api_key_openai, api_key_perplexity, api_key_mock, config_path, combined, quiet, no_metadata, timeout):
    """Thoth - AI-Powered Research Assistant
    
    Quick usage: thoth "QUERY"
    Advanced: thoth MODE "QUERY"
    Commands: init, status, list, providers, help [COMMAND]
    
    Examples:
      thoth "how does DNS work"
      thoth "best practices for REST APIs"
      thoth exploration "web frameworks"
      thoth help init
      thoth help status
    """
    # Set global config path if provided
    global _config_path
    if config_path:
        _config_path = Path(config_path).expanduser().resolve()
    
    # Show version if requested
    if version:
        console.print(f"Thoth v{THOTH_VERSION}")
        sys.exit(0)
    
    # Handle special commands
    if args and args[0] in ['init', 'status', 'list', 'help', 'providers']:
        command = args[0]
        if command == 'init':
            init_command()
            return
        elif command == 'status':
            if len(args) < 2:
                console.print("[red]Error:[/red] status command requires an operation ID")
                sys.exit(1)
            asyncio.run(show_status(args[1]))
            return
        elif command == 'list':
            show_all = '--all' in args
            asyncio.run(list_operations(show_all=show_all))
            return
        elif command == 'providers':
            # For providers command, we need to check both args and ctx.args 
            # because Click might intercept some flags
            all_args = list(args) + list(ctx.args)
            show_models = '--models' in all_args or '--models' in sys.argv
            show_list = '--list' in all_args or '--list' in sys.argv
            show_keys = '--keys' in all_args or '--keys' in sys.argv
            filter_provider = None
            
            # Check for --provider flag in multiple places
            for arg_list in [args, ctx.args, sys.argv]:
                for i, arg in enumerate(arg_list):
                    if arg in ['--provider', '-P'] and i + 1 < len(arg_list):
                        filter_provider = arg_list[i + 1]
                        break
                if filter_provider:
                    break
            
            # Also check if provider was parsed by Click
            if not filter_provider and provider:
                filter_provider = provider
                
            asyncio.run(providers_command(show_models=show_models, show_list=show_list, show_keys=show_keys, filter_provider=filter_provider))
            return
        elif command == 'help':
            # Check if there's a specific command to get help for
            if len(args) > 1:
                help_command = args[1]
                if help_command == 'init':
                    show_init_help()
                elif help_command == 'status':
                    show_status_help()
                elif help_command == 'list':
                    show_list_help()
                elif help_command == 'providers':
                    show_providers_help()
                else:
                    console.print(f"[red]Error:[/red] Unknown command: {help_command}")
                    console.print("[yellow]Available commands:[/yellow] init, status, list, providers")
                    console.print("\nUse 'thoth help' for general help")
            else:
                # Show general help - use Click's help instead
                console.print(ctx.get_help())
            return
    
    # For all other cases, treat as research query
    # Resolve mode and query from positional or options
    final_mode = None
    final_query = None
    
    if len(args) >= 2:
        # Two or more args: First is potentially a mode, rest is query
        # Check if first arg is a known mode
        if args[0] in BUILTIN_MODES:
            final_mode = args[0]
            final_query = " ".join(args[1:])
        else:
            # Check if it looks like a mode but is invalid
            # (heuristic: single word followed by more args)
            if len(args[0].split()) == 1 and not args[0].startswith("-"):
                # Might be an invalid mode
                console.print(f"[red]Error:[/red] Unknown mode: {args[0]}")
                console.print(f"[yellow]Available modes:[/yellow] {', '.join(BUILTIN_MODES.keys())}")
                sys.exit(1)
            # Not a mode, treat entire args as query
            final_mode = "default"
            final_query = " ".join(args)
    elif len(args) == 1:
        # One arg: could be just QUERY (default to default mode) or MODE (with --query)
        if query_opt:
            # If --query provided, arg is MODE
            final_mode = args[0]
            final_query = query_opt
        else:
            # Single arg is QUERY, use default mode
            final_mode = "default"
            final_query = args[0]
    else:
        # No positional args, check options
        final_mode = mode_opt or "default"
        final_query = query_opt
    # Handle query file
    if query_file:
        if query_file == '-':
            # Read from stdin with size limit
            stdin_data = sys.stdin.read(1024 * 1024)  # 1MB limit
            if len(stdin_data) >= 1024 * 1024:
                raise click.BadParameter("Stdin input exceeds 1MB limit")
            final_query = stdin_data.strip()
        else:
            with open(query_file, 'r') as f:
                final_query = f.read().strip()
    
    # Validation
    if async_mode and resume_id:
        raise click.BadParameter("Cannot use --async with --resume")
    
    if query_file and query_opt:
        raise click.BadParameter("Cannot use --query-file with --query")
        
    if input_file and auto:
        raise click.BadParameter("Cannot use --input-file with --auto")
    
    # Check for empty query
    if final_query is not None and final_query.strip() == "":
        raise click.BadParameter("Query cannot be empty")
    
    if resume_id:
        # Resume existing operation
        asyncio.run(resume_operation(resume_id, verbose))
    elif final_mode and final_query:
        # Run new research
        # Create provider-specific API keys dictionary
        cli_api_keys = {
            'openai': api_key_openai,
            'perplexity': api_key_perplexity,
            'mock': api_key_mock
        }
        asyncio.run(run_research(
            mode=final_mode,
            query=final_query,
            async_mode=async_mode,
            project=project,
            output_dir=output_dir,
            provider=provider,
            input_file=input_file,
            auto=auto,
            verbose=verbose,
            cli_api_keys=cli_api_keys,
            combined=combined,
            quiet=quiet,
            no_metadata=no_metadata,
            timeout_override=timeout
        ))
    else:
        # Show help if no valid command
        console.print(ctx.get_help())

def init_command():
    """Initialize Thoth configuration"""
    console.print("[bold]Welcome to Thoth Research Assistant Setup![/bold]\n")
    
    # Check environment
    console.print("Checking environment...")
    console.print(f"✓ Python {sys.version.split()[0]} detected")
    console.print("✓ UV package manager available")
    console.print(f"✓ Operating System: {sys.platform} (supported)\n")
    
    # Use custom config path if provided, otherwise use default
    if _config_path:
        config_path = _config_path
    else:
        config_path = Path(user_config_dir("thoth")) / "config.toml"
    console.print(f"Configuration file will be created at: {config_path}\n")
    
    # Create config directory if it doesn't exist
    config_path.parent.mkdir(parents=True, exist_ok=True)
    
    # TODO: Implement interactive setup wizard
    console.print("[yellow]Interactive setup wizard not yet implemented.[/yellow]")
    console.print("Creating default configuration...")
    
    # Create default config
    config = get_config()
    
    # For now, just save the default config using basic TOML format
    # Since we can't use external toml library in UV script, write manually
    with open(config_path, 'w') as f:
        f.write('# Thoth Configuration File\n')
        f.write(f'version = "{CONFIG_VERSION}"\n\n')
        f.write('[general]\n')
        f.write('default_project = ""\n')
        f.write('default_mode = "default"\n\n')
        f.write('[paths]\n')
        f.write('base_output_dir = "./research-outputs"\n')
        f.write(f'checkpoint_dir = "{config.data["paths"]["checkpoint_dir"]}"\n\n')
        f.write('[execution]\n')
        f.write('poll_interval = 30\n')
        f.write('max_wait = 30\n')
        f.write('parallel_providers = true\n')
        f.write('retry_attempts = 3\n')
        f.write('auto_input = true\n\n')
        f.write('[output]\n')
        f.write('combine_reports = false\n')
        f.write('format = "markdown"\n')
        f.write('include_metadata = true\n')
        f.write('timestamp_format = "%Y-%m-%d_%H%M%S"\n\n')
        f.write('[providers.openai]\n')
        f.write('api_key = "${OPENAI_API_KEY}"\n\n')
        f.write('[providers.perplexity]\n')
        f.write('api_key = "${PERPLEXITY_API_KEY}"\n')
    
    console.print(f"\n[green]✓[/green] Configuration saved to {config_path}")
    console.print("\nYou can now run: thoth deep_research \"your query\"")

def status_command(operation_id):
    """Check status of a research operation"""
    return show_status(operation_id)

def list_command(show_all):
    """List research operations"""
    return list_operations(show_all=show_all)

# ============================================================================
# Help Functions
# ============================================================================

def show_init_help():
    """Show detailed help for the init command"""
    console.print("\n[bold]thoth init[/bold] - Initialize Thoth configuration")
    console.print("\n[bold]Description:[/bold]")
    console.print("  Sets up Thoth configuration file and verifies environment.")
    console.print("  Creates default configuration at ~/.thoth/config.toml")
    console.print("\n[bold]Usage:[/bold]")
    console.print("  thoth init")
    console.print("\n[bold]What it does:[/bold]")
    console.print("  • Checks Python and UV package manager")
    console.print("  • Creates configuration directory")
    console.print("  • Generates default config.toml file")
    console.print("  • Sets up provider API key placeholders")
    console.print("\n[bold]Configuration file location:[/bold]")
    console.print(f"  {Path(user_config_dir('thoth')) / 'config.toml'}")
    console.print("\n[bold]After initialization:[/bold]")
    console.print("  1. Set your API keys as environment variables:")
    console.print("     export OPENAI_API_KEY='your-api-key'")
    console.print("     export PERPLEXITY_API_KEY='your-api-key'")
    console.print("  2. Or edit the config file directly")
    console.print("\n[bold]Examples:[/bold]")
    console.print("  # Initialize configuration")
    console.print("  $ thoth init")
    console.print("\n[bold]Related commands:[/bold]")
    console.print("  thoth help       - Show general help")

def show_status_help():
    """Show detailed help for the status command"""
    console.print("\n[bold]thoth status[/bold] - Check status of a research operation")
    console.print("\n[bold]Description:[/bold]")
    console.print("  Shows detailed status of a specific research operation,")
    console.print("  including progress, providers, and output files.")
    console.print("\n[bold]Usage:[/bold]")
    console.print("  thoth status <OPERATION_ID>")
    console.print("\n[bold]Arguments:[/bold]")
    console.print("  OPERATION_ID    The unique identifier for the operation")
    console.print("                  (e.g., research-20240803-143022-abc123...)")
    console.print("\n[bold]Information displayed:[/bold]")
    console.print("  • Operation ID and query")
    console.print("  • Current status (queued/running/completed/failed)")
    console.print("  • Start time and elapsed duration")
    console.print("  • Provider status for each LLM")
    console.print("  • Output file locations")
    console.print("\n[bold]Examples:[/bold]")
    console.print("  # Check status of a specific operation")
    console.print("  $ thoth status research-20240803-143022-1234abcd5678efgh")
    console.print("\n[bold]Related commands:[/bold]")
    console.print("  thoth list       - List all operations")
    console.print("  thoth help       - Show general help")

def show_list_help():
    """Show detailed help for the list command"""
    console.print("\n[bold]thoth list[/bold] - List research operations")
    console.print("\n[bold]Description:[/bold]")
    console.print("  Shows a table of research operations with their status.")
    console.print("  By default, shows only recent and active operations.")
    console.print("\n[bold]Usage:[/bold]")
    console.print("  thoth list [OPTIONS]")
    console.print("\n[bold]Options:[/bold]")
    console.print("  --all           Show all operations (not just recent/active)")
    console.print("\n[bold]Default behavior:[/bold]")
    console.print("  • Shows operations from the last 24 hours")
    console.print("  • Always shows running or queued operations")
    console.print("  • Sorted by creation time (newest first)")
    console.print("\n[bold]Table columns:[/bold]")
    console.print("  • ID       - Unique operation identifier")
    console.print("  • Query    - Research query (truncated if long)")
    console.print("  • Status   - Current status with color coding")
    console.print("  • Elapsed  - Time since operation started")
    console.print("  • Mode     - Research mode used")
    console.print("\n[bold]Examples:[/bold]")
    console.print("  # List recent operations")
    console.print("  $ thoth list")
    console.print("\n  # List all operations")
    console.print("  $ thoth list --all")
    console.print("\n[bold]Related commands:[/bold]")
    console.print("  thoth status     - Show details for specific operation")
    console.print("  thoth help       - Show general help")

def show_providers_help():
    """Show detailed help for the providers command"""
    console.print("\n[bold]thoth providers[/bold] - List providers and available models")
    console.print("\n[bold]Description:[/bold]")
    console.print("  Shows available providers and their configuration status,")
    console.print("  lists available models from each LLM provider,")
    console.print("  or displays API key configuration information.")
    console.print("  OpenAI models are fetched dynamically via API.")
    console.print("  Perplexity models are returned from a predefined list.")
    console.print("\n[bold]Usage:[/bold]")
    console.print("  thoth providers -- [OPTIONS]")
    console.print("\n[bold]Options:[/bold]")
    console.print("  --list                List available providers and their status")
    console.print("  --models              List available models from providers")
    console.print("  --keys                Show API key configuration for each provider")
    console.print("  --provider, -P        Filter by specific provider (with --models)")
    console.print("\n[bold]Note:[/bold]")
    console.print("  Use -- before options to prevent parsing conflicts")
    console.print("\n[bold]Available providers:[/bold]")
    console.print("  • openai     - OpenAI GPT models")
    console.print("  • perplexity - Perplexity Sonar models")
    console.print("  • mock       - Mock provider for testing")
    console.print("\n[bold]Examples:[/bold]")
    console.print("  # List all available providers")
    console.print("  $ thoth providers -- --list")
    console.print("\n  # Show API key configuration")
    console.print("  $ thoth providers -- --keys")
    console.print("\n  # List all models from all providers")
    console.print("  $ thoth providers -- --models")
    console.print("\n  # List only OpenAI models")
    console.print("  $ thoth providers -- --models --provider openai")
    console.print("  $ thoth providers -- --models -P openai")
    console.print("\n[bold]Related commands:[/bold]")
    console.print("  thoth help       - Show general help")

def show_general_help(ctx):
    """Show enhanced general help with command overview"""
    console.print("\n[bold]Thoth - AI-Powered Research Assistant[/bold]")
    console.print(f"Version {THOTH_VERSION}")
    console.print("\n[bold]Usage:[/bold]")
    console.print("  thoth [COMMAND] [OPTIONS]")
    console.print("  thoth [MODE] \"QUERY\" [OPTIONS]")
    console.print("  thoth \"QUERY\" [OPTIONS]")
    console.print("\n[bold]Quick Start:[/bold]")
    console.print("  # Simple query (uses default mode)")
    console.print("  $ thoth \"how does DNS work\"")
    console.print("\n  # Specify a research mode")
    console.print("  $ thoth deep_research \"explain kubernetes networking\"")
    console.print("\n[bold]Commands:[/bold]")
    console.print("  init            Initialize configuration")
    console.print("  status <ID>     Check operation status")
    console.print("  list            List research operations")
    console.print("  help [COMMAND]  Show help (general or command-specific)")
    console.print("\n[bold]Research Modes:[/bold]")
    for mode_name, mode_config in BUILTIN_MODES.items():
        desc = mode_config.get('description', 'No description')
        # Truncate long descriptions
        if len(desc) > 60:
            desc = desc[:57] + "..."
        console.print(f"  {mode_name:<15} {desc}")
    console.print("\n[bold]Common Options:[/bold]")
    console.print("  --mode, -m      Research mode to use")
    console.print("  --query, -q     Research query")
    console.print("  --async, -A     Submit and exit immediately")
    console.print("  --project, -p   Project name for organized output")
    console.print("  --verbose, -v   Show debug output")
    console.print("  --version, -V   Show version and exit")
    console.print("\n[bold]For detailed command help:[/bold]")
    console.print("  $ thoth help init")
    console.print("  $ thoth help status")
    console.print("  $ thoth help list")
    console.print("\n[bold]For all options:[/bold]")
    console.print("  $ thoth --help")

# ============================================================================
# Checkpoint Management
# ============================================================================

class CheckpointManager:
    """Handles operation persistence with corruption recovery"""
    
    def __init__(self, config: Config):
        self.checkpoint_dir = Path(config.data["paths"]["checkpoint_dir"])
        self.checkpoint_dir.mkdir(parents=True, exist_ok=True)
    
    async def save(self, operation: OperationStatus) -> None:
        """Save operation state atomically"""
        checkpoint_file = self.checkpoint_dir / f"{operation.id}.json"
        temp_file = checkpoint_file.with_suffix(".tmp")
        
        data = asdict(operation)
        # Convert datetime and Path objects to strings
        data["created_at"] = operation.created_at.isoformat()
        data["updated_at"] = operation.updated_at.isoformat()
        data["output_paths"] = {
            k: str(v) for k, v in operation.output_paths.items()
        }
        data["input_files"] = [str(p) for p in operation.input_files]
        
        async with aiofiles.open(temp_file, 'w') as f:
            await f.write(json.dumps(data, indent=2))
        
        temp_file.replace(checkpoint_file)
    
    async def load(self, operation_id: str) -> Optional[OperationStatus]:
        """Load operation from checkpoint with corruption handling"""
        checkpoint_file = self.checkpoint_dir / f"{operation_id}.json"
        
        if not checkpoint_file.exists():
            return None
        
        try:
            async with aiofiles.open(checkpoint_file, 'r') as f:
                data = json.loads(await f.read())
            
            # Convert back to proper types
            data["created_at"] = datetime.fromisoformat(data["created_at"])
            data["updated_at"] = datetime.fromisoformat(data["updated_at"])
            data["output_paths"] = {
                k: Path(v) for k, v in data["output_paths"].items()
            }
            data["input_files"] = [Path(p) for p in data.get("input_files", [])]
            
            return OperationStatus(**data)
        except (json.JSONDecodeError, KeyError, ValueError) as e:
            console.print(f"[yellow]Warning:[/yellow] Checkpoint file corrupted: {checkpoint_file}")
            console.print(f"[yellow]Creating new checkpoint. Previous state lost.[/yellow]")
            # Remove corrupted file
            checkpoint_file.unlink()
            return None
    
    def trigger_checkpoint(self, event: str) -> bool:
        """Determine if checkpoint should be saved based on event"""
        checkpoint_events = [
            "operation_start",
            "provider_start",
            "provider_complete",
            "provider_fail",
            "operation_complete",
            "operation_fail"
        ]
        return event in checkpoint_events

# ============================================================================
# Output Management
# ============================================================================

class OutputManager:
    """Manages research output files"""
    
    def __init__(self, config: Config, no_metadata: bool = False):
        self.config = config
        self.base_output_dir = Path(config.data["paths"]["base_output_dir"])
        self.format = config.data["output"]["format"]
        self.no_metadata = no_metadata
    
    def get_output_path(
        self,
        operation: OperationStatus,
        provider: str,
        output_dir: Optional[str] = None
    ) -> Path:
        """Generate output path based on mode"""
        timestamp = operation.created_at.strftime(
            self.config.data["output"]["timestamp_format"]
        )
        slug = sanitize_slug(operation.query)
        
        # Determine output directory
        if output_dir:
            # Explicit override - takes precedence over everything
            base_dir = Path(output_dir)
        elif operation.project:
            # Project mode
            base_dir = self.base_output_dir / operation.project
        else:
            # Ad-hoc mode - current directory
            base_dir = Path.cwd()
        
        # Ensure directory exists
        base_dir.mkdir(parents=True, exist_ok=True)
        
        # Generate filename with provider
        ext = "md" if self.format == "markdown" else "json"
        if provider == "combined":
            # Special case for combined reports: <timestamp>_<mode>_combined_<slug>.md
            base_name = f"{timestamp}_{operation.mode}_combined_{slug}"
        else:
            base_name = f"{timestamp}_{operation.mode}_{provider}_{slug}"
        filename = f"{base_name}.{ext}"
        
        # Handle deduplication
        output_path = base_dir / filename
        counter = 1
        while output_path.exists():
            filename = f"{base_name}-{counter}.{ext}"
            output_path = base_dir / filename
            counter += 1
        
        return output_path
    
    async def save_result(
        self,
        operation: OperationStatus,
        provider: str,
        content: str,
        output_dir: Optional[str] = None,
        model: Optional[str] = None,
        system_prompt: Optional[str] = None
    ) -> Path:
        """Save research result to file"""
        output_path = self.get_output_path(operation, provider, output_dir)
        
        # Check disk space before writing
        if not check_disk_space(output_path.parent, 10):  # 10MB minimum
            raise DiskSpaceError("Insufficient disk space to save results")
        
        if self.format == "markdown" and self.config.data["output"]["include_metadata"] and not self.no_metadata:
            # Add metadata header
            metadata = f"""---
query: {operation.query}
mode: {operation.mode}
provider: {provider}
model: {model if model else "Unknown"}
operation_id: {operation.id}
created_at: {operation.created_at.isoformat()}
"""
            if operation.input_files:
                metadata += "input_files:\n"
                for f in operation.input_files:
                    metadata += f"  - {f}\n"
            metadata += "---\n\n"
            
            # Add prompt section
            metadata += "### Prompt\n\n```\n"
            if system_prompt:
                metadata += f"System: {system_prompt}\n\nUser: {operation.query}\n"
            else:
                metadata += operation.query + "\n"
            metadata += "```\n\n"
            
            content = metadata + content
        
        # Write file
        async with aiofiles.open(output_path, 'w', encoding='utf-8') as f:
            await f.write(content)
        
        return output_path
    
    async def generate_combined_report(
        self,
        operation: OperationStatus,
        contents: Dict[str, str],
        output_dir: Optional[str] = None,
        system_prompt: Optional[str] = None
    ) -> Path:
        """Generate a combined report from multiple provider results"""
        # Create synthesized content
        combined_content = f"# Combined Research Report: {operation.query}\n\n"
        combined_content += f"Generated: {datetime.now().isoformat()}\n\n"
        
        for provider, content in contents.items():
            combined_content += f"\n## {provider.title()} Results\n\n"
            combined_content += content
            combined_content += "\n\n---\n\n"
        
        # Save combined report
        return await self.save_result(operation, "combined", combined_content, output_dir, 
                                     model="Multiple", system_prompt=system_prompt)

# ============================================================================
# Provider Base Classes and Implementations
# ============================================================================

class ResearchProvider:
    """Base class for research providers"""
    
    async def submit(self, query: str, mode: str, system_prompt: str = None) -> str:
        """Submit research and return job ID"""
        raise NotImplementedError
    
    async def check_status(self, job_id: str) -> Dict[str, Any]:
        """Check job status with progress information"""
        raise NotImplementedError
    
    async def get_result(self, job_id: str) -> str:
        """Get the final result content"""
        raise NotImplementedError
    
    def supports_progress(self) -> bool:
        """Whether this provider supports progress reporting"""
        return False
    
    async def list_models(self) -> List[Dict[str, Any]]:
        """List available models for this provider"""
        raise NotImplementedError

class MockProvider(ResearchProvider):
    """Mock provider for testing and development"""
    
    def __init__(self, name: str = "mock", delay: float = 0.1, api_key: str = ""):
        self.name = name
        self.delay = delay
        self.api_key = api_key
        self.model = "None"  # Mock provider has no model
        self.jobs = {}
    
    async def submit(self, query: str, mode: str, system_prompt: str = None) -> str:
        """Submit mock research and return job ID"""
        job_id = f"mock-{generate_operation_id()}"
        self.jobs[job_id] = {
            "query": query,
            "mode": mode,
            "status": "running",
            "progress": 0.0,
            "start_time": asyncio.get_event_loop().time()
        }
        return job_id
    
    async def check_status(self, job_id: str) -> Dict[str, Any]:
        """Check mock job status"""
        if job_id not in self.jobs:
            return {"status": "not_found", "error": "Job not found"}
        
        job = self.jobs[job_id]
        elapsed = asyncio.get_event_loop().time() - job["start_time"]
        
        if elapsed >= self.delay:
            job["status"] = "completed"
            job["progress"] = 1.0
        else:
            job["progress"] = min(elapsed / self.delay, 0.99)
        
        return {
            "status": job["status"],
            "progress": job["progress"],
            "elapsed": elapsed
        }
    
    async def get_result(self, job_id: str) -> str:
        """Get mock result"""
        if job_id not in self.jobs:
            raise ValueError("Job not found")
        
        job = self.jobs[job_id]
        return f"""# {self.name.title()} Research Results

## Query: {job["query"]}
## Mode: {job["mode"]}

This is a mock research result from the {self.name} provider.

### Key Findings
1. This is a simulated finding
2. Another mock insight
3. Test data point

### Conclusion
This mock provider successfully completed the research task.
"""
    
    def supports_progress(self) -> bool:
        return True
    
    async def list_models(self) -> List[Dict[str, Any]]:
        """Return mock models for testing"""
        return [
            {"id": "mock-model-v1", "created": 1680000000, "owned_by": "mock"},
            {"id": "mock-model-v2", "created": 1690000000, "owned_by": "mock"}
        ]

# Placeholder for real providers
class OpenAIProvider(ResearchProvider):
    """OpenAI Deep Research implementation"""
    
    def __init__(self, api_key: str, config: Dict[str, Any] = None):
        self.api_key = api_key
        self.config = config or {}
        self.model = self.config.get("model", "gpt-4o")  # Store model from config
        self.responses = {}  # Store responses by job_id
        
        # Add timeout configuration
        timeout = self.config.get("timeout", 30.0)
        self.client = AsyncOpenAI(
            api_key=api_key,
            timeout=httpx.Timeout(timeout, connect=5.0)
        )
    
    @retry(
        stop=stop_after_attempt(3),
        wait=wait_exponential(multiplier=1, min=4, max=10),
        retry=retry_if_exception_type((httpx.TimeoutException, httpx.ConnectError))
    )
    async def submit(self, query: str, mode: str, system_prompt: str = None) -> str:
        """Submit to OpenAI with retry logic for transient errors"""
        try:
            # Build messages
            messages = []
            if system_prompt:
                messages.append({"role": "system", "content": system_prompt})
            messages.append({"role": "user", "content": query})
            
            # Get configuration parameters
            temperature = self.config.get("temperature", 0.7)
            max_tokens = self.config.get("max_tokens", 4000)
            
            # Submit request using stored model
            response = await self.client.chat.completions.create(
                model=self.model,
                messages=messages,
                temperature=temperature,
                max_tokens=max_tokens
            )
            
            # Generate unique job ID
            job_id = f"openai-{datetime.now().strftime('%Y%m%d%H%M%S')}-{uuid4().hex[:8]}"
            
            # Store response by job_id
            self.responses[job_id] = response
            
            return job_id
            
        except httpx.TimeoutException:
            raise ProviderError("openai", "Request timed out. Try increasing timeout in config.")
        except httpx.ConnectError:
            raise ProviderError("openai", "Failed to connect to OpenAI API. Check your internet connection.")
        except Exception as e:
            error_msg = str(e)
            error_msg_lower = error_msg.lower()
            
            # Check for authentication errors (401)
            if "authentication" in error_msg_lower or "api key" in error_msg_lower or "401" in error_msg:
                if "incorrect api key" in error_msg_lower:
                    raise ThothError(
                        "Invalid OpenAI API key",
                        "Please check your API key at https://platform.openai.com/account/api-keys"
                    )
                else:
                    raise APIKeyError("openai")
            
            # Check for rate limit errors (429)
            elif "rate limit" in error_msg_lower or "429" in error_msg:
                raise ProviderError(
                    "openai", 
                    "Rate limit exceeded. Please wait a moment and try again."
                )
            
            # Check for model not found errors (404)
            elif "model" in error_msg_lower and ("not found" in error_msg_lower or "404" in error_msg):
                raise ProviderError(
                    "openai",
                    f"Model '{self.model}' not found. Please check available models with 'thoth providers -- --models --provider openai'"
                )
            
            # Check for quota exceeded errors
            elif "quota" in error_msg_lower or "insufficient_quota" in error_msg_lower:
                raise ProviderError(
                    "openai",
                    "API quota exceeded. Please check your OpenAI account billing."
                )
            
            # Check for invalid request errors (400)
            elif "invalid" in error_msg_lower or "400" in error_msg:
                raise ProviderError(
                    "openai",
                    f"Invalid request: {error_msg}"
                )
            
            # Generic error
            else:
                raise ProviderError("openai", str(e))
    
    async def check_status(self, job_id: str) -> Dict[str, Any]:
        """Check status - for sync mode, always return completed"""
        return {
            "status": "completed",
            "progress": 1.0
        }
    
    async def get_result(self, job_id: str) -> str:
        """Get the result"""
        if job_id in self.responses:
            response = self.responses[job_id]
            return response.choices[0].message.content
        return "No response available"
    
    async def list_models(self) -> List[Dict[str, Any]]:
        """Fetch available models from OpenAI API"""
        try:
            response = await self.client.models.list()
            models = []
            for model in response.data:
                # Filter to only include text generation models
                if any(cap in model.id for cap in ['gpt', 'o1', 'davinci', 'curie', 'babbage', 'ada']):
                    models.append({
                        "id": model.id,
                        "created": model.created,
                        "owned_by": model.owned_by
                    })
            return sorted(models, key=lambda x: x['created'], reverse=True)
        except Exception as e:
            raise ProviderError("openai", f"Failed to fetch models: {str(e)}")

class PerplexityProvider(ResearchProvider):
    """Perplexity research implementation"""
    
    def __init__(self, api_key: str, config: Dict[str, Any] = None):
        self.api_key = api_key
        self.config = config or {}
        self.model = self.config.get("model", "sonar-pro")  # Store model from config
    
    async def submit(self, query: str, mode: str, system_prompt: str = None) -> str:
        """Submit to Perplexity"""
        # TODO: Implement actual Perplexity submission
        raise NotImplementedError("Perplexity provider not yet implemented")
    
    async def list_models(self) -> List[Dict[str, Any]]:
        """Return hardcoded Perplexity models as specified"""
        return [
            {"id": "sonar-deep-research", "created": 1700000000, "owned_by": "perplexity"}
        ]

# ============================================================================
# Provider Factory
# ============================================================================

def create_provider(provider_name: str, config: Config, cli_api_key: Optional[str] = None, timeout_override: Optional[float] = None) -> ResearchProvider:
    """Create a provider instance with proper configuration and error handling"""
    provider_config = config.data["providers"].get(provider_name, {})
    
    if provider_name == "mock":
        # Mock provider accepts any API key (for testing)
        # Check environment variable first
        mock_api_key = os.getenv("MOCK_API_KEY", "")
        
        # Override with CLI key if provided
        if cli_api_key:
            mock_api_key = cli_api_key
        
        # Override with config key if no env/cli key
        if not mock_api_key:
            mock_api_key = provider_config.get("api_key", "")
        
        # Mock provider requires some API key (can be dummy)
        if not mock_api_key or (mock_api_key.startswith("${") and mock_api_key.endswith("}")):
            raise APIKeyError("mock")
        
        # Special validation for testing: reject "invalid" as API key
        if mock_api_key == "invalid":
            raise ThothError("Invalid mock API key format", "Mock API key should not be 'invalid'")
        
        return MockProvider(name=provider_name, delay=0.1, api_key=mock_api_key)
    
    # Get API key from provider config
    api_key = provider_config.get("api_key", "")
    
    # Override with CLI key if provided
    if cli_api_key:
        api_key = cli_api_key
    
    # Check if API key exists or is still a placeholder
    if not api_key or (api_key.startswith("${") and api_key.endswith("}")):
        raise APIKeyError(provider_name)
    
    # Apply timeout override if provided
    if timeout_override is not None and provider_name in ['openai', 'perplexity']:
        provider_config = provider_config.copy()  # Don't modify original config
        provider_config['timeout'] = timeout_override
    
    # Create provider instance
    if provider_name == "openai":
        return OpenAIProvider(api_key=api_key, config=provider_config)
    elif provider_name == "perplexity":
        return PerplexityProvider(api_key=api_key, config=provider_config)
    else:
        raise ThothError(f"Unknown provider: {provider_name}", "Valid providers are: openai, perplexity, mock")

# ============================================================================
# Async Functions for Research Operations
# ============================================================================

async def find_latest_outputs(current_mode: str, project: Optional[str], config: Config) -> List[Path]:
    """Find latest outputs from previous mode in chain"""
    mode_config = config.get_mode_config(current_mode)
    previous_mode = mode_config.get("previous")
    
    if not previous_mode:
        return []
    
    # Determine search directory
    if project:
        search_dir = Path(config.data["paths"]["base_output_dir"]) / project
    else:
        search_dir = Path.cwd()
    
    if not search_dir.exists():
        return []
    
    # Use strict pattern matching to avoid false matches
    # Pattern: YYYY-MM-DD_HHMMSS_<mode>_<provider>_<slug>.(md|json)
    pattern = f"*_*_{previous_mode}_*_*.md"
    files = sorted(search_dir.glob(pattern), key=lambda p: p.stat().st_mtime, reverse=True)
    
    # Validate files match expected pattern more strictly
    valid_files = []
    for file in files:
        parts = file.stem.split('_')
        if len(parts) >= 5:  # Ensure we have all expected parts
            # Validate timestamp format
            try:
                datetime.strptime(f"{parts[0]}_{parts[1]}", "%Y-%m-%d_%H%M%S")
                if parts[2] == previous_mode:  # Confirm mode matches
                    valid_files.append(file)
            except ValueError:
                continue
    
    # Return latest file for each provider
    providers_found = set()
    result = []
    for file in valid_files:
        parts = file.stem.split('_')
        if len(parts) >= 4:
            provider = parts[3]
            # Skip combined reports when collecting inputs
            if provider != "combined" and provider not in providers_found:
                providers_found.add(provider)
                result.append(file)
    
    return result

def get_estimated_duration(mode: str, provider: Optional[str]) -> float:
    """Get estimated duration in seconds based on mode and provider"""
    estimates = {
        "thinking": {"openai": 10, "perplexity": 8, "mock": 5},
        "clarification": {"openai": 15, "perplexity": 12, "mock": 5},
        "exploration": {"openai": 20, "perplexity": 15, "mock": 10},
        "deep_research": {"openai": 22, "perplexity": 20, "combined": 23, "mock": 2}
    }
    
    mode_estimates = estimates.get(mode, {"default": 60})
    if provider:
        return mode_estimates.get(provider, 60)
    else:
        # For multi-provider modes, use the max time
        return mode_estimates.get("combined", max(mode_estimates.values(), default=60))

# ============================================================================
# Async Functions (Main implementations)
# ============================================================================

async def run_research(mode: str, query: str, async_mode: bool, 
                      project: Optional[str], 
                      output_dir: Optional[str], provider: Optional[str],
                      input_file: Optional[str], auto: bool, verbose: bool,
                      cli_api_keys: Optional[Dict[str, Optional[str]]] = None, combined: bool = False,
                      quiet: bool = False, no_metadata: bool = False, timeout_override: Optional[float] = None):
    """Execute research operation"""
    global _current_checkpoint_manager, _current_operation
    
    config = get_config()
    
    # Check disk space
    output_path = Path(output_dir) if output_dir else Path.cwd()
    
    # Create output directory if it doesn't exist
    if output_dir and not output_path.exists():
        output_path.mkdir(parents=True, exist_ok=True)
    
    if not check_disk_space(output_path):
        raise DiskSpaceError("Insufficient disk space", "Free up at least 100MB")
    
    # Validate that mode and query are provided
    if not mode or not query:
        raise click.BadParameter("Both mode and query are required for research operations")
    
    # Create operation
    operation_id = generate_operation_id()
    
    if verbose:
        console.print(f"[dim]Operation ID: {operation_id}[/dim]")
    
    # Handle input files
    input_files = []
    if input_file:
        input_files.append(Path(input_file))
    elif auto:
        # Check mode-specific auto_input first, then global
        mode_config = config.get_mode_config(mode)
        mode_auto = mode_config.get("auto_input")
        global_auto = config.data["execution"].get("auto_input", False)
        
        if mode_auto is not None:
            use_auto = mode_auto
        else:
            use_auto = global_auto
            
        if use_auto:
            # Find latest relevant output
            input_files = await find_latest_outputs(mode, project, config)
            if not input_files:
                console.print("[yellow]Warning:[/yellow] No previous outputs found for auto-input")
    
    # Create operation status
    operation = OperationStatus(
        id=operation_id,
        query=query,
        mode=mode,
        status="queued",
        created_at=datetime.now(),
        updated_at=datetime.now(),
        project=project,
        input_files=input_files
    )
    
    # Initialize managers
    checkpoint_manager = CheckpointManager(config)
    output_manager = OutputManager(config, no_metadata=no_metadata)
    
    # Set globals for signal handling
    _current_checkpoint_manager = checkpoint_manager
    _current_operation = operation
    
    # Save initial checkpoint
    await checkpoint_manager.save(operation)
    
    # Get providers to use
    mode_config = config.get_mode_config(mode)
    
    # Determine which providers to use
    if provider:
        # When --provider is explicitly specified, use only that provider
        # This takes precedence over any mode configuration
        providers_to_use = [provider]
    elif mode == "thinking" or "provider" in mode_config:
        # Single-provider mode (only if no explicit --provider flag)
        default_provider = mode_config.get("provider", "openai")
        providers_to_use = [default_provider]
    elif "providers" in mode_config:
        # Multi-provider mode (only if no explicit --provider flag)
        providers_to_use = mode_config.get("providers", ["openai"])
    else:
        # Default to single provider
        providers_to_use = ["openai"]
    
    
    # Create provider instances
    providers = {}
    for provider_name in providers_to_use:
        try:
            # Get the provider-specific CLI API key if provided
            provider_cli_key = None
            if cli_api_keys:
                provider_cli_key = cli_api_keys.get(provider_name)
            providers[provider_name] = create_provider(provider_name, config, cli_api_key=provider_cli_key, timeout_override=timeout_override)
        except APIKeyError as e:
            console.print(f"[red]Error:[/red] {e.message}")
            console.print(f"[yellow]Suggestion:[/yellow] {e.suggestion}")
            raise click.Abort()
        except ThothError as e:
            console.print(f"[red]Error:[/red] {e.message}")
            console.print(f"[yellow]Suggestion:[/yellow] {e.suggestion}")
            raise click.Abort()
    
    # Show masked API keys and timeout in verbose mode
    if verbose:
        for provider_name, provider_instance in providers.items():
            if hasattr(provider_instance, 'api_key') and provider_instance.api_key:
                masked = mask_api_key(provider_instance.api_key)
                console.print(f"[dim]{provider_name} API Key: {masked}[/dim]")
            # Show timeout for providers that support it
            if hasattr(provider_instance, 'config') and 'timeout' in provider_instance.config:
                timeout_value = provider_instance.config.get('timeout')
                console.print(f"[dim]{provider_name} Timeout: {timeout_value}s[/dim]")
    
    if async_mode:
        # Submit and exit
        if not quiet:
            console.print(f"[green]✓[/green] Research submitted")
        console.print(f"Operation ID: [bold]{operation_id}[/bold]")
        if not quiet:
            console.print(f"\nCheck status: [dim]thoth status {operation_id}[/dim]")
        
        # TODO: Actually submit async tasks
        return
    
    # Synchronous execution with progress
    from rich.progress import Progress, SpinnerColumn, TextColumn, BarColumn, TimeElapsedColumn
    
    with Progress(
        SpinnerColumn(),
        TextColumn("[progress.description]{task.description}"),
        BarColumn(),
        TextColumn("[progress.percentage]{task.percentage:>3.0f}%"),
        TimeElapsedColumn(),
        TextColumn("| Next poll: {task.fields[next_poll]}s"),
        console=console,
        disable=quiet  # Disable progress in quiet mode
    ) as progress:
        # Submit to all providers
        jobs = {}
        for provider_name, provider_instance in providers.items():
            system_prompt = mode_config.get("system_prompt", "")
            job_id = await provider_instance.submit(query, mode, system_prompt)
            jobs[provider_name] = {
                "provider": provider_instance,
                "job_id": job_id,
                "task_id": progress.add_task(f"{provider_name.title()} Research", total=100, next_poll=30)
            }
            operation.providers[provider_name] = {"status": "running", "job_id": job_id}
        
        # Update checkpoint
        operation.status = "running"
        await checkpoint_manager.save(operation)
        
        # Poll for completion
        all_completed = False
        poll_interval = config.data["execution"]["poll_interval"]
        max_wait = config.data["execution"]["max_wait"] * 60  # Convert to seconds
        start_time = asyncio.get_event_loop().time()
        
        while not all_completed:
            all_completed = True
            
            for provider_name, job_info in jobs.items():
                status = await job_info["provider"].check_status(job_info["job_id"])
                
                if status["status"] == "running":
                    all_completed = False
                    progress_pct = int(status.get("progress", 0) * 100)
                    elapsed = asyncio.get_event_loop().time() - start_time
                    next_poll = max(0, poll_interval - int(elapsed % poll_interval))
                    progress.update(job_info["task_id"], completed=progress_pct, next_poll=next_poll)
                elif status["status"] == "completed":
                    progress.update(job_info["task_id"], completed=100, next_poll=0)
                    
                    # Get and save result
                    if provider_name not in operation.output_paths:
                        result_content = await job_info["provider"].get_result(job_info["job_id"])
                        # Get model information from provider
                        provider_model = getattr(job_info["provider"], "model", None)
                        # Get system prompt from mode config
                        system_prompt = mode_config.get("system_prompt", "")
                        output_path = await output_manager.save_result(
                            operation, provider_name, result_content, output_dir,
                            model=provider_model, system_prompt=system_prompt
                        )
                        operation.output_paths[provider_name] = output_path
                        operation.providers[provider_name]["status"] = "completed"
                        await checkpoint_manager.save(operation)
            
            # Check timeout
            if asyncio.get_event_loop().time() - start_time > max_wait:
                console.print(f"\n[red]Timeout exceeded ({max_wait/60} minutes)[/red]")
                break
            
            if not all_completed:
                await asyncio.sleep(min(poll_interval, 1))  # Sleep for 1 second increments
    
    # Generate combined report if multiple providers and combined flag is set
    if len(operation.output_paths) > 1 and combined:
        contents = {}
        for provider_name, path in operation.output_paths.items():
            with open(path, 'r') as f:
                contents[provider_name] = f.read()
        
        combined_path = await output_manager.generate_combined_report(
            operation, contents, output_dir, system_prompt=mode_config.get("system_prompt", "")
        )
        operation.output_paths["combined"] = combined_path
    
    # Final checkpoint
    operation.status = "completed"
    operation.updated_at = datetime.now()
    await checkpoint_manager.save(operation)
    
    if not quiet:
        console.print(f"\n[green]✓[/green] Research completed!")
        if project:
            console.print(f"Results saved to: [dim]{config.data['paths']['base_output_dir']}/{project}/[/dim]")
        else:
            console.print(f"Results saved to: [dim]current directory[/dim]")
    
    # Show output files
    for provider_name, path in operation.output_paths.items():
        console.print(f"  • {path.name}")
    
    # Suggest combined report if multiple providers and not already generated
    if not quiet and len(operation.output_paths) > 1 and "combined" not in operation.output_paths:
        console.print("\nTo generate a combined report, run with --combined flag")
    
    # Clear globals after successful completion
    _current_checkpoint_manager = None
    _current_operation = None
    
    return operation

async def resume_operation(operation_id: str, verbose: bool):
    """Resume an existing operation"""
    config = get_config()
    checkpoint_manager = CheckpointManager(config)
    
    # Load operation from checkpoint
    operation = await checkpoint_manager.load(operation_id)
    if not operation:
        console.print(f"[red]Error:[/red] Operation {operation_id} not found")
        sys.exit(6)
    
    console.print(f"[yellow]Resuming operation {operation_id}...[/yellow]")
    console.print(f"Query: {operation.query}")
    console.print(f"Mode: {operation.mode}")
    console.print(f"Status: {operation.status}")
    
    # TODO: Implement actual resumption logic
    console.print("[yellow]Full resume functionality not yet implemented[/yellow]")

async def show_status(operation_id: str):
    """Show status of a specific operation"""
    config = get_config()
    checkpoint_manager = CheckpointManager(config)
    
    # Load operation from checkpoint
    operation = await checkpoint_manager.load(operation_id)
    if not operation:
        console.print(f"[red]Error:[/red] Operation {operation_id} not found")
        sys.exit(6)
    
    # Display operation details
    console.print("\nOperation Details:")
    console.print("─────────────────")
    console.print(f"ID:        {operation.id}")
    console.print(f"Query:     {operation.query}")
    console.print(f"Mode:      {operation.mode}")
    console.print(f"Status:    {operation.status}")
    console.print(f"Started:   {operation.created_at.strftime('%Y-%m-%d %H:%M:%S')}")
    
    # Calculate elapsed time
    if operation.status in ["running", "completed"]:
        elapsed = datetime.now() - operation.created_at
        minutes = int(elapsed.total_seconds() / 60)
        console.print(f"Elapsed:   {minutes} minutes")
    
    if operation.project:
        console.print(f"Project:   {operation.project}")
    
    # Show provider status
    if operation.providers:
        console.print("\nProvider Status:")
        console.print("───────────────")
        for provider_name, provider_info in operation.providers.items():
            status_icon = "✓" if provider_info.get("status") == "completed" else "▶"
            status_text = provider_info.get("status", "unknown").title()
            console.print(f"{provider_name.title()}:  {status_icon} {status_text}")
    
    # Show output files
    if operation.output_paths:
        console.print("\nOutput Files:")
        console.print("────────────")
        if operation.project:
            base_dir = Path(config.data["paths"]["base_output_dir"]) / operation.project
            console.print(f"{base_dir}/")
        else:
            console.print("./")
        
        for provider_name, path in operation.output_paths.items():
            console.print(f"  ├── {Path(path).name}")

async def list_operations(show_all: bool):
    """List all operations"""
    config = get_config()
    checkpoint_manager = CheckpointManager(config)
    
    # Get all checkpoint files
    checkpoint_files = list(checkpoint_manager.checkpoint_dir.glob("*.json"))
    
    if not checkpoint_files:
        console.print("No operations found.")
        return
    
    # Load operations
    operations = []
    for checkpoint_file in checkpoint_files:
        operation_id = checkpoint_file.stem
        operation = await checkpoint_manager.load(operation_id)
        if operation:
            operations.append(operation)
    
    # Sort by creation time (newest first)
    operations.sort(key=lambda op: op.created_at, reverse=True)
    
    # Filter if not showing all
    if not show_all:
        # Only show recent or active operations
        cutoff_time = datetime.now() - timedelta(hours=24)
        operations = [op for op in operations if op.status in ["running", "queued"] or op.created_at > cutoff_time]
    
    if not operations:
        console.print("No active operations found. Use --all to see all operations.")
        return
    
    # Display table
    from rich.table import Table
    
    table = Table(title="Research Operations")
    table.add_column("ID", style="dim", width=40)
    table.add_column("Query", width=25)
    table.add_column("Status", width=10)
    table.add_column("Elapsed", width=8)
    table.add_column("Mode", width=15)
    
    for operation in operations:
        # Truncate query if too long
        query_display = operation.query[:22] + "..." if len(operation.query) > 25 else operation.query
        
        # Calculate elapsed time
        elapsed = datetime.now() - operation.created_at
        if elapsed.total_seconds() < 3600:
            elapsed_str = f"{int(elapsed.total_seconds() / 60)}m"
        else:
            elapsed_str = f"{int(elapsed.total_seconds() / 3600)}h"
        
        # Status color
        status_style = "green" if operation.status == "completed" else "yellow" if operation.status == "running" else "dim"
        
        table.add_row(
            operation.id,
            query_display,
            f"[{status_style}]{operation.status}[/{status_style}]",
            elapsed_str,
            operation.mode
        )
    
    console.print(table)
    console.print("\nUse 'thoth status <ID>' for details")

async def providers_command(show_models: bool = False, show_list: bool = False, show_keys: bool = False, filter_provider: str = None):
    """Show provider information and available models"""
    if not show_models and not show_list and not show_keys:
        console.print("[yellow]Usage:[/yellow] thoth providers -- [OPTIONS]")
        console.print("\nShow provider information and available models.")
        console.print("\nOptions:")
        console.print("  --list                List available providers and their status")
        console.print("  --models              List available models from providers")
        console.print("  --keys                Show API key configuration for each provider")
        console.print("  --provider, -P        Filter by specific provider (with --models)")
        console.print("\n[dim]Note: Use -- before options to prevent parsing conflicts[/dim]")
        console.print("\nExamples:")
        console.print("  # List all available providers")
        console.print("  $ thoth providers -- --list")
        console.print("\n  # Show API key configuration")
        console.print("  $ thoth providers -- --keys")
        console.print("\n  # List all models from all providers")
        console.print("  $ thoth providers -- --models")
        console.print("\n  # List only OpenAI models")
        console.print("  $ thoth providers -- --models --provider openai")
        return
    
    config = get_config()
    
    # Define available providers
    all_providers = ['openai', 'perplexity', 'mock']
    
    # Provider descriptions
    provider_descriptions = {
        'openai': 'OpenAI GPT models',
        'perplexity': 'Perplexity search AI',
        'mock': 'Mock provider for tests'
    }
    
    if show_list:
        # Show provider listing with status
        table = Table(title="Available Providers", box=box.ROUNDED)
        table.add_column("Provider", style="cyan", width=13)
        table.add_column("Status", width=10)
        table.add_column("Description", style="dim", width=25)
        
        for provider_name in all_providers:
            try:
                # Try to create provider instance to check if configured
                provider = create_provider(provider_name, config)
                status = "[green]✓ Ready[/green]"
            except APIKeyError:
                status = "[red]✗ No key[/red]"
            except Exception:
                status = "[yellow]⚠ Error[/yellow]"
            
            description = provider_descriptions.get(provider_name, "Unknown provider")
            table.add_row(provider_name, status, description)
        
        console.print(table)
        console.print("\nTo see available models, use: thoth providers -- --models")
        return
    
    if show_keys:
        # Show API key configuration
        # Hardcoded mapping of providers to environment variables
        env_vars = {
            'openai': 'OPENAI_API_KEY',
            'perplexity': 'PERPLEXITY_API_KEY',
            'mock': 'MOCK_API_KEY'
        }
        
        table = Table(title="Provider API Key Configuration", box=box.ROUNDED)
        table.add_column("Provider", style="cyan", width=13)
        table.add_column("Environment Variable", style="green", width=22)
        table.add_column("CLI Argument", style="yellow", width=22)
        
        for provider_name in all_providers:
            env_var = env_vars.get(provider_name, f"{provider_name.upper()}_API_KEY")
            cli_arg = f"--api-key-{provider_name}"
            table.add_row(provider_name, env_var, cli_arg)
        
        console.print(table)
        console.print("\nExamples:")
        console.print("  # Set via environment variable")
        console.print("  $ export OPENAI_API_KEY=\"your-key-here\"")
        console.print("\n  # Set via command line for single provider")
        console.print("  $ thoth \"query\" --api-key-openai \"your-key-here\" --provider openai")
        console.print("\n  # Set multiple API keys for multi-provider modes")
        console.print("  $ thoth deep_research \"query\" --api-key-openai \"sk-...\" --api-key-perplexity \"pplx-...\"")
        return
    
    # Original --models logic
    providers = all_providers
    if filter_provider:
        if filter_provider not in providers:
            # Print to stderr directly
            print(f"Error: Unknown provider: {filter_provider}", file=sys.stderr)
            print(f"Available providers: {', '.join(providers)}", file=sys.stderr)
            sys.exit(1)
        providers = [filter_provider]
    
    console.print("Fetching available models...\n")
    
    for provider_name in providers:
        try:
            # Create provider instance
            provider = create_provider(provider_name, config)
            
            # Fetch models
            models = await provider.list_models()
            
            # Calculate dynamic column width based on longest model ID
            if models:
                max_model_id_length = max(len(model.get('id', '')) for model in models)
                # Add padding and ensure minimum width
                model_id_width = max(max_model_id_length + 2, 20)
            else:
                model_id_width = 25  # Default width if no models
            
            # Display in table format
            if provider_name == 'openai':
                table = Table(title=f"OpenAI Models", box=box.ROUNDED)
                table.add_column("Model ID", style="cyan", width=model_id_width)
                table.add_column("Created", style="green", width=14)
                table.add_column("Owned By", style="yellow", width=16)
                
                for model in models:
                    created_date = datetime.fromtimestamp(model['created']).strftime('%Y-%m-%d')
                    table.add_row(
                        model['id'],
                        created_date,
                        model['owned_by']
                    )
            elif provider_name == 'perplexity':
                table = Table(title=f"Perplexity Models", box=box.ROUNDED)
                table.add_column("Model ID", style="cyan", width=model_id_width)
                
                for model in models:
                    table.add_row(model['id'])
            else:  # mock
                table = Table(title=f"Mock Models", box=box.ROUNDED)
                table.add_column("Model ID", style="cyan", width=model_id_width)
                
                for model in models:
                    table.add_row(model['id'])
            
            console.print(table)
            console.print()
            
        except APIKeyError as e:
            console.print(f"[yellow]Skipping {provider_name}:[/yellow] {e}")
            console.print()
        except Exception as e:
            console.print(f"[red]Error fetching models from {provider_name}:[/red] {str(e)}")
            console.print()

# ============================================================================
# Signal Handlers
# ============================================================================

def handle_sigint(signum, frame):
    """Handle Ctrl-C gracefully by saving checkpoint"""
    console.print("\n[yellow]Interrupt received. Saving checkpoint...[/yellow]")
    
    if _current_checkpoint_manager and _current_operation:
        # Update operation status
        _current_operation.status = "interrupted"
        _current_operation.updated_at = datetime.now()
        
        # Save checkpoint synchronously
        try:
            # Run async save in sync context
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
            loop.run_until_complete(_current_checkpoint_manager.save(_current_operation))
            console.print(f"[green]✓[/green] Checkpoint saved. Resume with: thoth --resume {_current_operation.id}")
        except Exception as e:
            console.print(f"[red]Error saving checkpoint:[/red] {e}")
        finally:
            loop.close()
    
    console.print("[red]Exiting...[/red]")
    sys.exit(1)

# ============================================================================
# Main Entry Point
# ============================================================================

if __name__ == "__main__":
    # Register signal handler for graceful shutdown
    signal.signal(signal.SIGINT, handle_sigint)
    
    try:
        cli()
    except Exception as e:
        import traceback
        traceback.print_exc()
        handle_error(e)